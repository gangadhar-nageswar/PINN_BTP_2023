{"cells":[{"cell_type":"markdown","metadata":{"id":"llx0I8S9ix-2"},"source":["# **Libraries and Dependencies**"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9150,"status":"ok","timestamp":1666152687323,"user":{"displayName":"Nageswar Venkata Sai Gangadhar","userId":"02855835275532238793"},"user_tz":-330},"id":"FZU2tjpvjPL6","outputId":"bf7c439b-a09a-4bed-dc95-9e868805fb11"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyDOE2 in /usr/local/lib/python3.7/dist-packages (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyDOE2) (1.7.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyDOE2) (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: plotting in /usr/local/lib/python3.7/dist-packages (0.0.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from plotting) (1.3.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from plotting) (3.2.2)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from plotting) (0.11.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plotting) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plotting) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plotting) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plotting) (0.11.0)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plotting) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->plotting) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->plotting) (1.15.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->plotting) (2022.4)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->plotting) (1.7.3)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pygad in /usr/local/lib/python3.7/dist-packages (2.18.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pygad) (1.21.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pygad) (3.2.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pygad) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pygad) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pygad) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pygad) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->pygad) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->pygad) (1.15.0)\n"]}],"source":["! pip install pyDOE2 #Latin Hypercube Sampling\n","! pip install plotting\n","! pip install pygad"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4504,"status":"ok","timestamp":1666152691819,"user":{"displayName":"Nageswar Venkata Sai Gangadhar","userId":"02855835275532238793"},"user_tz":-330},"id":"ps3AZqnu9XBo","outputId":"f047b911-18be-47a4-ed54-f1ec6f7cadfc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1666152691820,"user":{"displayName":"Nageswar Venkata Sai Gangadhar","userId":"02855835275532238793"},"user_tz":-330},"id":"wh0mH9W9h6W7"},"outputs":[],"source":["import sys\n","sys.path.insert(0, '../Utilities/')\n","\n","import torch\n","from collections import OrderedDict\n","\n","from pyDOE2 import lhs\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.io\n","from scipy.interpolate import griddata\n","# from plotting import newfig, savefig\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","import matplotlib.gridspec as gridspec\n","import time\n","\n","np.random.seed(1234)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1666152691820,"user":{"displayName":"Nageswar Venkata Sai Gangadhar","userId":"02855835275532238793"},"user_tz":-330},"id":"mLg6_gcWiQM_"},"outputs":[],"source":["# CUDA support \n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')"]},{"cell_type":"markdown","metadata":{"id":"I597Le6yi4F5"},"source":["# **Physics-informed Neural Networks**"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1666152691821,"user":{"displayName":"Nageswar Venkata Sai Gangadhar","userId":"02855835275532238793"},"user_tz":-330},"id":"rg2zU76XiRBB"},"outputs":[],"source":["# the deep neural network\n","class DNN(torch.nn.Module):\n","    def __init__(self, layers):\n","        super(DNN, self).__init__()\n","        \n","        # parameters\n","        self.depth = len(layers) - 1\n","        \n","        # set up layer order dict\n","        self.activation = torch.nn.Tanh\n","        \n","        layer_list = list()\n","        for i in range(self.depth - 1): \n","            layer = torch.nn.Linear(layers[i], layers[i+1])\n","            torch.nn.init.xavier_normal_(layer.weight.data, gain=1.0)\n","            torch.nn.init.zeros_(layer.bias.data)\n","\n","            layer_list.append(\n","                ('layer_%d' % i, layer)\n","            )\n","            layer_list.append(('activation_%d' % i, self.activation()))\n","            # layer_list.append(('dropout_%d'%i, torch.nn.Dropout(0.005)))\n","\n","            # layer_list.append(('normalisation_%d' % i, torch.nn.BatchNorm1d(layers[i+1])))\n","            # layer_list.append(('normalisation_%d' % i, torch.nn.layerNorm(layers[i+1])))\n","\n","        layer_list.append(\n","            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n","        )\n","        layerDict = OrderedDict(layer_list)\n","        \n","        # deploy layers\n","        self.layers = torch.nn.Sequential(layerDict)\n","        \n","    def forward(self, x):\n","        out = self.layers(x)\n","        return out\n"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1666152691821,"user":{"displayName":"Nageswar Venkata Sai Gangadhar","userId":"02855835275532238793"},"user_tz":-330},"id":"-dVFg-gciRTs"},"outputs":[],"source":["# the physics-guided neural network\n","class PhysicsInformedNN():\n","    def __init__(self, X_u, u, X_f, layers, lb, ub, nu, X_star):\n","        \n","        # boundary conditions\n","        self.lb = torch.tensor(lb).float().to(device)\n","        self.ub = torch.tensor(ub).float().to(device)\n","        \n","        self.nu = X_u.shape[0]\n","        self.nf = X_f.shape[0]\n","\n","        # data\n","        self.x_u = torch.tensor(X_u[:, 0:1], requires_grad=True).float().to(device)\n","        self.t_u = torch.tensor(X_u[:, 1:2], requires_grad=True).float().to(device)\n","        self.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device)\n","        self.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n","        self.u = torch.tensor(u).float().to(device)\n","\n","        # # grid points\n","        self.num_x = 256\n","        self.num_t = 100\n","        self.build_grid()\n","\n","        self.layers = layers\n","        self.nu = nu\n","        \n","        # deep neural networks\n","        self.dnn = DNN(layers).to(device)\n","        \n","        # optimizers: using the same settings\n","        # the below implementation is intended for discritising the steps involved in optimiser.step()\n","        self.optimizer = torch.optim.LBFGS(\n","            self.dnn.parameters(), \n","            lr=1.0, \n","            max_iter=20, \n","            max_eval=20, \n","            history_size=50,\n","            tolerance_grad=1e-5, \n","            tolerance_change=1.0 * np.finfo(float).eps,\n","            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n","        )\n","\n","        self.num_epochs = 400\n","        self.iter = 0\n","\n","        self.set_adaptive_col_params()\n","\n","        #random seeds\n","        self.lhs_rs = 0\n","        torch.manual_seed(0)\n","        np.random.seed(1234)\n","\n","    def build_grid(self):\n","        grid_x = np.zeros(shape=(1, self.num_x))\n","        grid_x[0] = lb[0] + ((ub[0]-lb[0])/(self.num_x - 1))*np.arange(self.num_x)\n","        grid_t = np.zeros(shape=(1, self.num_t))\n","        grid_t[0] = lb[1] + ((ub[1]-lb[1])/(self.num_t - 1))*np.arange(self.num_t)\n","\n","        gX, gT = np.meshgrid(grid_x, grid_t)\n","        G = np.hstack((gX.flatten()[:,None], gT.flatten()[:,None]))\n","\n","        self.grid_x = torch.tensor(G[:, 0:1], requires_grad=True).float().to(device)\n","        self.grid_t = torch.tensor(G[:, 1:2], requires_grad=True).float().to(device)      \n","\n","    def set_adaptive_col_params(self):\n","        self.proxy = \"adapt-r\"\n","        self.gamma = 0.0\n","        self.T = 100\n","        self.e = 10\n","\n","    def net_u(self, x, t):\n","        X = torch.cat([x, t], dim=1)\n","\n","        # X = torch.tensor([x, t], requires_grad=True).float().to(device)\n","        # X = torch.tensor([-0.5, 0.5], requires_grad=True).float().to(device)\n","\n","        u = self.dnn(X)\n","        return u\n","\n","    def net_f(self, x, t):\n","        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n","        u = self.net_u(x, t)\n","        \n","        u_t = torch.autograd.grad(\n","            u, t, \n","            grad_outputs=torch.ones_like(u),\n","            retain_graph=True,\n","            create_graph=True,\n","            allow_unused=True\n","        )[0]\n","\n","        u_x = torch.autograd.grad(\n","            u, x, \n","            grad_outputs=torch.ones_like(u),\n","            retain_graph=True,\n","            create_graph=True,\n","            allow_unused=True\n","        )[0]\n","\n","        u_xx = torch.autograd.grad(\n","            u_x, x, \n","            grad_outputs=torch.ones_like(u_x),\n","            retain_graph=True,\n","            create_graph=True,\n","            allow_unused=True\n","        )[0]\n","        \n","        f = u_t + u * u_x - self.nu * u_xx\n","\n","        return f\n","    \n","    def grad_value(self, x, t):\n","        u = self.net_u(x, t)\n","        \n","        u_t = torch.autograd.grad(\n","            u, t, \n","            grad_outputs=torch.ones_like(u),\n","            retain_graph=True,\n","            create_graph=True,\n","            allow_unused=True\n","        )[0]\n","\n","        u_x = torch.autograd.grad(\n","            u, x, \n","            grad_outputs=torch.ones_like(u),\n","            retain_graph=True,\n","            create_graph=True,\n","            allow_unused=True\n","        )[0]\n","        \n","        grad_tensor = torch.sqrt(torch.square(u_x) + torch.square(u_t))\n","        \n","        return grad_tensor\n","\n","    def proxy_func(self):\n","        prob = np.zeros(shape=(self.num_x, self.num_t))\n","\n","        start = time.time()\n","        \n","        if self.proxy == 'adapt-r':\n","            prob_proxy = self.net_f(self.grid_x, self.grid_t)\n","        elif self.proxy == 'adapt-g':\n","            prob_proxy = self.grad_value(self.grid_x, self.grid_t)\n","\n","        prob = np.transpose(np.reshape(abs(prob_proxy).cpu().detach().numpy(), (self.num_t, self.num_x)))\n","\n","        # for j in range(self.num_t):\n","        #   for i in range(self.num_x):\n","        #     prob[i][j] = (abs(prob_proxy[j*self.num_x + i][0])).cpu().detach().numpy()\n","        \n","        # prob = abs(res).cpu().detach().numpy()\n","\n","        return prob/prob.sum()\n","\n","    def sample_uniform(self, size):\n","\n","        x_f_train1 = self.lb[0].item() + (self.ub[0].item() - self.lb[0].item())*lhs(1, size, random_state=self.lhs_rs)\n","        t_f_train1 = self.lb[1].item() + (self.ub[1].item() - self.lb[1].item())*lhs(1, size)\n","\n","        return x_f_train1, t_f_train1\n","    \n","    def sample_proxy(self, size):\n","\n","        # normalise the array again\n","        self.prob /= self.prob.sum()\n","        flat = self.prob.flatten()\n","\n","        x_f_train2 = np.zeros(shape=(size, 1))\n","        t_f_train2 = np.zeros(shape=(size, 1))\n","\n","        for s in range(size):\n","            sample_index = np.random.choice(a=flat.size, p=flat)\n","            index = np.unravel_index(sample_index, self.prob.shape)\n","\n","            x = self.lb[0] + (self.ub[0] - self.lb[0]) * index[0] / (self.num_x)\n","            t = self.lb[1] + (self.ub[1] - self.lb[1]) * index[1] / (self.num_t)\n","\n","            x_f_train2[s][0] = x\n","            t_f_train2[s][0] = t\n","\n","        return x_f_train2, t_f_train2\n","\n","    def loss_func(self):\n","\n","        global train_error\n","        \n","        self.optimizer.zero_grad()\n","        \n","        u_pred = self.net_u(self.x_u, self.t_u)\n","        f_pred = self.net_f(self.x_f, self.t_f)\n","\n","        self.loss_u = torch.mean((self.u - u_pred) ** 2)\n","        self.loss_f = torch.mean(f_pred ** 2)\n","        \n","        loss = self.loss_u + self.loss_f\n","        \n","        loss.backward()\n","\n","        self.iter += 1\n","        # if self.iter % 100 == 0:\n","        print(\n","            'Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), self.loss_u.item(), self.loss_f.item())\n","        )\n","        \n","        train_error.append(loss.item())\n","\n","        return loss\n","    \n","    def adaptive_train(self):\n","        # set the model training mode \"on\"\n","        self.dnn.train()\n","\n","        self.prob = np.ones(shape=(self.num_x, self.num_t)) / (self.num_x * self.num_t)\n","\n","        for epoch_index in range(self.num_epochs):\n","            new_prob = self.proxy_func()\n","            self.prob = new_prob + self.gamma * self.prob\n","            self.prob /= self.prob.sum()\n","\n","            self.Tc = epoch_index % self.T\n","            self.eta = 1/2 * (1 + np.cos(np.pi * self.Tc/self.T))\n","\n","            if epoch_index % self.e == 0:\n","                # sample (self.eta * Nf) xf uniformly\n","                x_f1, t_f1 = self.sample_uniform(int(self.eta * self.nf))\n","                \n","                # sample (1-self.eta)*Nf points of xf using proxy function Pi\n","                x_f2, t_f2 = self.sample_proxy(int((1-self.eta)*self.nf))\n","\n","                self.x_f = np.concatenate((x_f1, x_f2), axis=0)\n","                self.t_f = np.concatenate((t_f1, t_f2), axis=0)\n","\n","                self.x_f = torch.tensor(self.x_f, requires_grad=True).float().to(device)\n","                self.t_f = torch.tensor(self.t_f, requires_grad=True).float().to(device)\n","\n","\n","            # Backward and optimize\n","            self.optimizer.step(self.loss_func)\n","            \n","            # # get a test error\n","            # global test_error\n","            # u_pred, f_pred = model.predict(X_star)\n","\n","            # error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n","            # test_error.append(error_u)\n","        \n","        return None\n","    \n","    def vanilla_train(self):\n","        # set the model training mode \"on\"\n","        self.dnn.train()\n","\n","        for epoch_index in range(self.num_epochs):\n","            self.optimizer.step(self.loss_func)\n","    \n","            \n","    def predict(self, X):\n","        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n","        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n","\n","        self.dnn.eval()\n","        u = self.net_u(x, t)\n","        f = self.net_f(x, t)\n","        u = u.detach().cpu().numpy()\n","        f = f.detach().cpu().numpy()\n","        return u, f"]},{"cell_type":"markdown","metadata":{"id":"8wkb6umMi8h_"},"source":["# **Configurations**"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1666152691821,"user":{"displayName":"Nageswar Venkata Sai Gangadhar","userId":"02855835275532238793"},"user_tz":-330},"id":"hCbkmjYliRZW","outputId":"8551cb6b-a956-4e70-b83c-ddc7d0c351ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["type of t: <class 'numpy.ndarray'> || shape: (100, 1)\n","type of X: <class 'numpy.ndarray'> || shape: (100, 256)\n","type of T: <class 'numpy.ndarray'> || shape: (100, 256)\n","type of X_star: <class 'numpy.ndarray'> || shape: (25600, 2)\n"]}],"source":["nu = 0.01/np.pi\n","noise = 0.0        \n","\n","train_error = []\n","test_error = []\n","\n","N_u = 100\n","N_f = 100\n","n_neurs = 20\n","layers = [2, n_neurs, n_neurs, n_neurs, n_neurs, n_neurs, n_neurs, n_neurs, n_neurs, 1]\n","\n","data = scipy.io.loadmat('/content/drive/MyDrive/SEM - 7/BTP/data/burgers_shock.mat')\n","\n","t = data['t'].flatten()[:,None]\n","x = data['x'].flatten()[:,None]\n","\n","print(\"type of t: {} || shape: {}\".format(type(t), t.shape))\n","\n","Exact = np.real(data['usol']).T\n","\n","X, T = np.meshgrid(x,t)\n","print(\"type of X: {} || shape: {}\".format(type(X), X.shape))\n","print(\"type of T: {} || shape: {}\".format(type(T), T.shape))\n","\n","X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n","print(\"type of X_star: {} || shape: {}\".format(type(X_star), X_star.shape))\n","\n","u_star = Exact.flatten()[:,None]              \n","\n","# Doman bounds\n","lb = X_star.min(0)\n","ub = X_star.max(0)\n","mu = X_star.mean()\n","var = X_star.var()    \n","\n","xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n","uu1 = Exact[0:1,:].T\n","xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n","uu2 = Exact[:,0:1]\n","xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n","uu3 = Exact[:,-1:]\n","\n","X_u_train = np.vstack([xx1, xx2, xx3])\n","\n","X_f_train = lb + (ub-lb)*lhs(2, N_f)\n","# X_f_train = (lhs(2, N_f) - mu) / var\n","\n","X_f_train = np.vstack((X_f_train, X_u_train))\n","u_train = np.vstack([uu1, uu2, uu3])\n","\n","idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n","X_u_train = X_u_train[idx, :]\n","u_train = u_train[idx,:]\n","\n","#u_pred, f_pred = model.predict(X_star)\n","\n","#error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n","#print('Error u: %e' % (error_u))                     \n","\n","#U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n","#Error = np.abs(Exact - U_pred)"]},{"cell_type":"markdown","metadata":{"id":"c1fi_ecfjB0m"},"source":["# **Training**"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1666152691821,"user":{"displayName":"Nageswar Venkata Sai Gangadhar","userId":"02855835275532238793"},"user_tz":-330},"id":"fZgoy9FcXoYq","outputId":"a71326bd-dafa-4ac2-b153-4d437c8a1a6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["-1.0 1.0\n","0.0 0.99\n"]}],"source":["print(lb[0], ub[0])\n","print(lb[1], ub[1])"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":533,"status":"ok","timestamp":1666152903812,"user":{"displayName":"Nageswar Venkata Sai Gangadhar","userId":"02855835275532238793"},"user_tz":-330},"id":"L_hnkbjQiReL"},"outputs":[],"source":["model = PhysicsInformedNN(X_u_train, u_train, X_f_train, layers, lb, ub, nu, X_star)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":18645,"status":"ok","timestamp":1666152924691,"user":{"displayName":"Nageswar Venkata Sai Gangadhar","userId":"02855835275532238793"},"user_tz":-330},"id":"r3E8I_xEiRid","outputId":"4e35b15c-2570-4d71-b135-2e7159524ab3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Iter 1, Loss: 2.87851e-01, Loss_u: 2.87697e-01, Loss_f: 1.54375e-04\n","Iter 2, Loss: 2.65901e-01, Loss_u: 2.65660e-01, Loss_f: 2.41178e-04\n","Iter 3, Loss: 2.04742e-01, Loss_u: 1.91875e-01, Loss_f: 1.28670e-02\n","Iter 4, Loss: 1.99442e-01, Loss_u: 1.94218e-01, Loss_f: 5.22422e-03\n","Iter 5, Loss: 1.98763e-01, Loss_u: 1.92923e-01, Loss_f: 5.83983e-03\n","Iter 6, Loss: 1.98017e-01, Loss_u: 1.91916e-01, Loss_f: 6.10129e-03\n","Iter 7, Loss: 1.96198e-01, Loss_u: 1.89697e-01, Loss_f: 6.50104e-03\n","Iter 8, Loss: 1.97092e-01, Loss_u: 1.93307e-01, Loss_f: 3.78505e-03\n","Iter 9, Loss: 1.84260e-01, Loss_u: 1.80001e-01, Loss_f: 4.25940e-03\n","Iter 10, Loss: 1.85849e-01, Loss_u: 1.82368e-01, Loss_f: 3.48107e-03\n","Iter 11, Loss: 1.80297e-01, Loss_u: 1.76328e-01, Loss_f: 3.96876e-03\n","Iter 12, Loss: 1.63788e-01, Loss_u: 1.54677e-01, Loss_f: 9.11155e-03\n","Iter 13, Loss: 1.57970e-01, Loss_u: 1.46987e-01, Loss_f: 1.09823e-02\n","Iter 14, Loss: 1.50063e-01, Loss_u: 1.35790e-01, Loss_f: 1.42726e-02\n","Iter 15, Loss: 1.48536e-01, Loss_u: 1.36382e-01, Loss_f: 1.21540e-02\n","Iter 16, Loss: 1.47399e-01, Loss_u: 1.35737e-01, Loss_f: 1.16612e-02\n","Iter 17, Loss: 1.46515e-01, Loss_u: 1.33870e-01, Loss_f: 1.26450e-02\n","Iter 18, Loss: 1.44271e-01, Loss_u: 1.25832e-01, Loss_f: 1.84392e-02\n","Iter 19, Loss: 1.42294e-01, Loss_u: 1.23052e-01, Loss_f: 1.92414e-02\n","Iter 20, Loss: 1.35737e-01, Loss_u: 9.73222e-02, Loss_f: 3.84143e-02\n","Iter 21, Loss: 1.35737e-01, Loss_u: 9.73222e-02, Loss_f: 3.84143e-02\n","Iter 22, Loss: 1.26915e-01, Loss_u: 1.08240e-01, Loss_f: 1.86758e-02\n","Iter 23, Loss: 1.30796e-01, Loss_u: 1.10718e-01, Loss_f: 2.00783e-02\n","Iter 24, Loss: 1.25438e-01, Loss_u: 1.06700e-01, Loss_f: 1.87377e-02\n","Iter 25, Loss: 1.24165e-01, Loss_u: 1.00369e-01, Loss_f: 2.37953e-02\n","Iter 26, Loss: 1.23523e-01, Loss_u: 1.01001e-01, Loss_f: 2.25219e-02\n","Iter 27, Loss: 1.22045e-01, Loss_u: 1.01113e-01, Loss_f: 2.09327e-02\n","Iter 28, Loss: 1.18309e-01, Loss_u: 9.65611e-02, Loss_f: 2.17480e-02\n","Iter 29, Loss: 2.17991e-01, Loss_u: 1.02481e-01, Loss_f: 1.15510e-01\n","Iter 30, Loss: 1.15456e-01, Loss_u: 8.42253e-02, Loss_f: 3.12306e-02\n","Iter 31, Loss: 1.14945e-01, Loss_u: 8.35272e-02, Loss_f: 3.14180e-02\n","Iter 32, Loss: 1.14803e-01, Loss_u: 8.31175e-02, Loss_f: 3.16851e-02\n","Iter 33, Loss: 1.14855e-01, Loss_u: 8.52508e-02, Loss_f: 2.96045e-02\n","Iter 34, Loss: 1.13908e-01, Loss_u: 8.36016e-02, Loss_f: 3.03068e-02\n","Iter 35, Loss: 1.14735e-01, Loss_u: 7.87795e-02, Loss_f: 3.59556e-02\n","Iter 36, Loss: 1.13309e-01, Loss_u: 8.16051e-02, Loss_f: 3.17041e-02\n","Iter 37, Loss: 1.12845e-01, Loss_u: 7.99541e-02, Loss_f: 3.28911e-02\n","Iter 38, Loss: 1.11933e-01, Loss_u: 7.87650e-02, Loss_f: 3.31685e-02\n","Iter 39, Loss: 1.09837e-01, Loss_u: 7.22685e-02, Loss_f: 3.75689e-02\n","Iter 40, Loss: 1.09056e-01, Loss_u: 7.72509e-02, Loss_f: 3.18051e-02\n","Iter 41, Loss: 1.09056e-01, Loss_u: 7.72509e-02, Loss_f: 3.18051e-02\n","Iter 42, Loss: 1.08117e-01, Loss_u: 7.51699e-02, Loss_f: 3.29469e-02\n","Iter 43, Loss: 1.06150e-01, Loss_u: 7.30093e-02, Loss_f: 3.31411e-02\n","Iter 44, Loss: 1.04637e-01, Loss_u: 6.90961e-02, Loss_f: 3.55409e-02\n","Iter 45, Loss: 1.06761e-01, Loss_u: 7.82866e-02, Loss_f: 2.84748e-02\n","Iter 46, Loss: 1.03002e-01, Loss_u: 7.10181e-02, Loss_f: 3.19837e-02\n","Iter 47, Loss: 1.02833e-01, Loss_u: 7.27394e-02, Loss_f: 3.00932e-02\n","Iter 48, Loss: 1.02352e-01, Loss_u: 7.31068e-02, Loss_f: 2.92448e-02\n","Iter 49, Loss: 1.02199e-01, Loss_u: 7.21702e-02, Loss_f: 3.00291e-02\n","Iter 50, Loss: 1.01750e-01, Loss_u: 7.16880e-02, Loss_f: 3.00623e-02\n","Iter 51, Loss: 1.01020e-01, Loss_u: 7.12553e-02, Loss_f: 2.97643e-02\n","Iter 52, Loss: 1.00414e-01, Loss_u: 7.17147e-02, Loss_f: 2.86991e-02\n","Iter 53, Loss: 1.00014e-01, Loss_u: 7.20560e-02, Loss_f: 2.79584e-02\n","Iter 54, Loss: 9.98950e-02, Loss_u: 7.11820e-02, Loss_f: 2.87130e-02\n","Iter 55, Loss: 9.97838e-02, Loss_u: 7.14933e-02, Loss_f: 2.82905e-02\n","Iter 56, Loss: 9.95000e-02, Loss_u: 7.10296e-02, Loss_f: 2.84705e-02\n","Iter 57, Loss: 9.89151e-02, Loss_u: 6.87957e-02, Loss_f: 3.01194e-02\n","Iter 58, Loss: 9.84346e-02, Loss_u: 6.80976e-02, Loss_f: 3.03370e-02\n","Iter 59, Loss: 9.83225e-02, Loss_u: 6.66514e-02, Loss_f: 3.16711e-02\n","Iter 60, Loss: 1.00984e-01, Loss_u: 7.15060e-02, Loss_f: 2.94777e-02\n","Iter 61, Loss: 9.82240e-02, Loss_u: 6.72804e-02, Loss_f: 3.09437e-02\n","Iter 62, Loss: 9.82240e-02, Loss_u: 6.72804e-02, Loss_f: 3.09437e-02\n","Iter 63, Loss: 9.89807e-02, Loss_u: 6.96209e-02, Loss_f: 2.93598e-02\n","Iter 64, Loss: 9.81281e-02, Loss_u: 6.77632e-02, Loss_f: 3.03649e-02\n","Iter 65, Loss: 9.80997e-02, Loss_u: 6.89261e-02, Loss_f: 2.91736e-02\n","Iter 66, Loss: 9.79611e-02, Loss_u: 6.82622e-02, Loss_f: 2.96989e-02\n","Iter 67, Loss: 9.79179e-02, Loss_u: 6.75744e-02, Loss_f: 3.03435e-02\n","Iter 68, Loss: 9.78297e-02, Loss_u: 6.78946e-02, Loss_f: 2.99351e-02\n","Iter 69, Loss: 9.75508e-02, Loss_u: 6.84969e-02, Loss_f: 2.90539e-02\n","Iter 70, Loss: 9.70550e-02, Loss_u: 6.92539e-02, Loss_f: 2.78012e-02\n","Iter 71, Loss: 9.66079e-02, Loss_u: 6.96555e-02, Loss_f: 2.69524e-02\n","Iter 72, Loss: 9.63957e-02, Loss_u: 6.92698e-02, Loss_f: 2.71259e-02\n","Iter 73, Loss: 9.59035e-02, Loss_u: 6.88152e-02, Loss_f: 2.70883e-02\n","Iter 74, Loss: 9.59325e-02, Loss_u: 6.38512e-02, Loss_f: 3.20813e-02\n","Iter 75, Loss: 9.55090e-02, Loss_u: 6.61490e-02, Loss_f: 2.93601e-02\n","Iter 76, Loss: 9.55501e-02, Loss_u: 6.55423e-02, Loss_f: 3.00078e-02\n","Iter 77, Loss: 9.53864e-02, Loss_u: 6.58658e-02, Loss_f: 2.95206e-02\n","Iter 78, Loss: 9.51268e-02, Loss_u: 6.51320e-02, Loss_f: 2.99948e-02\n","Iter 79, Loss: 9.48694e-02, Loss_u: 6.55597e-02, Loss_f: 2.93097e-02\n","Iter 80, Loss: 9.47213e-02, Loss_u: 6.50925e-02, Loss_f: 2.96288e-02\n","Iter 81, Loss: 9.46655e-02, Loss_u: 6.42779e-02, Loss_f: 3.03877e-02\n","Iter 82, Loss: 9.46655e-02, Loss_u: 6.42779e-02, Loss_f: 3.03877e-02\n","Iter 83, Loss: 9.46442e-02, Loss_u: 6.41338e-02, Loss_f: 3.05104e-02\n","Iter 84, Loss: 9.46166e-02, Loss_u: 6.40028e-02, Loss_f: 3.06139e-02\n","Iter 85, Loss: 9.44853e-02, Loss_u: 6.37388e-02, Loss_f: 3.07465e-02\n","Iter 86, Loss: 9.42090e-02, Loss_u: 6.42189e-02, Loss_f: 2.99901e-02\n","Iter 87, Loss: 9.40627e-02, Loss_u: 6.39549e-02, Loss_f: 3.01077e-02\n","Iter 88, Loss: 9.38366e-02, Loss_u: 6.59200e-02, Loss_f: 2.79166e-02\n","Iter 89, Loss: 9.37440e-02, Loss_u: 6.55660e-02, Loss_f: 2.81780e-02\n","Iter 90, Loss: 9.37092e-02, Loss_u: 6.55015e-02, Loss_f: 2.82077e-02\n","Iter 91, Loss: 9.35769e-02, Loss_u: 6.55480e-02, Loss_f: 2.80290e-02\n","Iter 92, Loss: 9.33022e-02, Loss_u: 6.55911e-02, Loss_f: 2.77111e-02\n","Iter 93, Loss: 9.28925e-02, Loss_u: 6.49014e-02, Loss_f: 2.79912e-02\n","Iter 94, Loss: 9.27188e-02, Loss_u: 6.51177e-02, Loss_f: 2.76012e-02\n","Iter 95, Loss: 9.26039e-02, Loss_u: 6.89636e-02, Loss_f: 2.36403e-02\n","Iter 96, Loss: 9.24894e-02, Loss_u: 6.73152e-02, Loss_f: 2.51742e-02\n","Iter 97, Loss: 9.23916e-02, Loss_u: 6.75119e-02, Loss_f: 2.48796e-02\n","Iter 98, Loss: 9.21623e-02, Loss_u: 6.76470e-02, Loss_f: 2.45153e-02\n","Iter 99, Loss: 9.17289e-02, Loss_u: 6.65908e-02, Loss_f: 2.51381e-02\n","Iter 100, Loss: 9.13207e-02, Loss_u: 6.86058e-02, Loss_f: 2.27149e-02\n","Iter 101, Loss: 9.10195e-02, Loss_u: 6.59992e-02, Loss_f: 2.50203e-02\n","Iter 102, Loss: 9.10195e-02, Loss_u: 6.59992e-02, Loss_f: 2.50203e-02\n","Iter 103, Loss: 9.07398e-02, Loss_u: 6.84351e-02, Loss_f: 2.23047e-02\n","Iter 104, Loss: 9.03946e-02, Loss_u: 6.71384e-02, Loss_f: 2.32562e-02\n","Iter 105, Loss: 9.04938e-02, Loss_u: 6.94167e-02, Loss_f: 2.10772e-02\n","Iter 106, Loss: 9.02573e-02, Loss_u: 6.79449e-02, Loss_f: 2.23124e-02\n","Iter 107, Loss: 8.99782e-02, Loss_u: 6.65836e-02, Loss_f: 2.33946e-02\n","Iter 108, Loss: 8.92123e-02, Loss_u: 6.39437e-02, Loss_f: 2.52686e-02\n","Iter 109, Loss: 8.83216e-02, Loss_u: 6.41740e-02, Loss_f: 2.41476e-02\n","Iter 110, Loss: 8.78677e-02, Loss_u: 6.34888e-02, Loss_f: 2.43789e-02\n","Iter 111, Loss: 8.71578e-02, Loss_u: 6.55940e-02, Loss_f: 2.15638e-02\n","Iter 112, Loss: 8.68251e-02, Loss_u: 6.67313e-02, Loss_f: 2.00937e-02\n","Iter 113, Loss: 8.56030e-02, Loss_u: 6.59519e-02, Loss_f: 1.96511e-02\n","Iter 114, Loss: 8.75371e-02, Loss_u: 7.16641e-02, Loss_f: 1.58731e-02\n","Iter 115, Loss: 8.49012e-02, Loss_u: 6.70812e-02, Loss_f: 1.78200e-02\n","Iter 116, Loss: 8.32606e-02, Loss_u: 6.55610e-02, Loss_f: 1.76996e-02\n","Iter 117, Loss: 8.16471e-02, Loss_u: 5.90934e-02, Loss_f: 2.25536e-02\n","Iter 118, Loss: 8.25692e-02, Loss_u: 6.03096e-02, Loss_f: 2.22596e-02\n","Iter 119, Loss: 8.11369e-02, Loss_u: 5.90567e-02, Loss_f: 2.20803e-02\n","Iter 120, Loss: 8.06198e-02, Loss_u: 5.93672e-02, Loss_f: 2.12526e-02\n","Iter 121, Loss: 8.01499e-02, Loss_u: 5.70390e-02, Loss_f: 2.31108e-02\n","Iter 122, Loss: 8.01499e-02, Loss_u: 5.70390e-02, Loss_f: 2.31108e-02\n","Iter 123, Loss: 7.98212e-02, Loss_u: 5.69740e-02, Loss_f: 2.28471e-02\n","Iter 124, Loss: 7.96816e-02, Loss_u: 5.70233e-02, Loss_f: 2.26583e-02\n","Iter 125, Loss: 7.94497e-02, Loss_u: 5.70613e-02, Loss_f: 2.23884e-02\n","Iter 126, Loss: 7.89416e-02, Loss_u: 5.57672e-02, Loss_f: 2.31744e-02\n","Iter 127, Loss: 7.85604e-02, Loss_u: 5.48930e-02, Loss_f: 2.36673e-02\n","Iter 128, Loss: 7.82770e-02, Loss_u: 5.49298e-02, Loss_f: 2.33472e-02\n","Iter 129, Loss: 7.78365e-02, Loss_u: 5.44093e-02, Loss_f: 2.34272e-02\n","Iter 130, Loss: 7.75291e-02, Loss_u: 5.39957e-02, Loss_f: 2.35334e-02\n","Iter 131, Loss: 7.73174e-02, Loss_u: 5.45278e-02, Loss_f: 2.27895e-02\n","Iter 132, Loss: 7.67604e-02, Loss_u: 5.37155e-02, Loss_f: 2.30449e-02\n","Iter 133, Loss: 7.57862e-02, Loss_u: 5.40406e-02, Loss_f: 2.17457e-02\n","Iter 134, Loss: 7.43876e-02, Loss_u: 5.34050e-02, Loss_f: 2.09825e-02\n","Iter 135, Loss: 7.32372e-02, Loss_u: 5.12952e-02, Loss_f: 2.19420e-02\n","Iter 136, Loss: 7.26212e-02, Loss_u: 5.34007e-02, Loss_f: 1.92205e-02\n","Iter 137, Loss: 7.17502e-02, Loss_u: 5.01906e-02, Loss_f: 2.15596e-02\n","Iter 138, Loss: 7.12987e-02, Loss_u: 5.04690e-02, Loss_f: 2.08296e-02\n","Iter 139, Loss: 7.08560e-02, Loss_u: 4.87930e-02, Loss_f: 2.20630e-02\n","Iter 140, Loss: 7.04837e-02, Loss_u: 4.90672e-02, Loss_f: 2.14166e-02\n","Iter 141, Loss: 7.03742e-02, Loss_u: 4.92639e-02, Loss_f: 2.11103e-02\n","Iter 142, Loss: 7.03742e-02, Loss_u: 4.92639e-02, Loss_f: 2.11103e-02\n","Iter 143, Loss: 7.02213e-02, Loss_u: 4.92815e-02, Loss_f: 2.09398e-02\n","Iter 144, Loss: 6.98819e-02, Loss_u: 4.94883e-02, Loss_f: 2.03936e-02\n","Iter 145, Loss: 6.99480e-02, Loss_u: 4.91227e-02, Loss_f: 2.08253e-02\n","Iter 146, Loss: 6.97013e-02, Loss_u: 4.93066e-02, Loss_f: 2.03947e-02\n","Iter 147, Loss: 6.94795e-02, Loss_u: 4.88452e-02, Loss_f: 2.06342e-02\n","Iter 148, Loss: 6.93654e-02, Loss_u: 4.87588e-02, Loss_f: 2.06065e-02\n","Iter 149, Loss: 6.88628e-02, Loss_u: 4.84202e-02, Loss_f: 2.04427e-02\n","Iter 150, Loss: 6.78448e-02, Loss_u: 4.85023e-02, Loss_f: 1.93425e-02\n","Iter 151, Loss: 7.10291e-02, Loss_u: 5.24702e-02, Loss_f: 1.85589e-02\n","Iter 152, Loss: 6.72180e-02, Loss_u: 4.91748e-02, Loss_f: 1.80432e-02\n","Iter 153, Loss: 7.03812e-02, Loss_u: 5.13391e-02, Loss_f: 1.90421e-02\n","Iter 154, Loss: 6.65786e-02, Loss_u: 4.96806e-02, Loss_f: 1.68980e-02\n","Iter 155, Loss: 6.54742e-02, Loss_u: 4.90029e-02, Loss_f: 1.64713e-02\n","Iter 156, Loss: 6.49993e-02, Loss_u: 4.67408e-02, Loss_f: 1.82584e-02\n","Iter 157, Loss: 6.45371e-02, Loss_u: 4.65176e-02, Loss_f: 1.80195e-02\n","Iter 158, Loss: 6.43942e-02, Loss_u: 4.68194e-02, Loss_f: 1.75749e-02\n","Iter 159, Loss: 6.42135e-02, Loss_u: 4.58561e-02, Loss_f: 1.83574e-02\n","Iter 160, Loss: 6.39462e-02, Loss_u: 4.47663e-02, Loss_f: 1.91799e-02\n","Iter 161, Loss: 6.36221e-02, Loss_u: 4.45931e-02, Loss_f: 1.90291e-02\n","Iter 162, Loss: 6.36221e-02, Loss_u: 4.45931e-02, Loss_f: 1.90291e-02\n","Iter 163, Loss: 6.32649e-02, Loss_u: 4.46508e-02, Loss_f: 1.86140e-02\n","Iter 164, Loss: 6.28219e-02, Loss_u: 4.45067e-02, Loss_f: 1.83151e-02\n","Iter 165, Loss: 6.20689e-02, Loss_u: 4.49178e-02, Loss_f: 1.71511e-02\n","Iter 166, Loss: 6.11327e-02, Loss_u: 4.43495e-02, Loss_f: 1.67831e-02\n","Iter 167, Loss: 6.01174e-02, Loss_u: 4.52629e-02, Loss_f: 1.48545e-02\n","Iter 168, Loss: 5.89770e-02, Loss_u: 4.35493e-02, Loss_f: 1.54277e-02\n","Iter 169, Loss: 6.11075e-02, Loss_u: 4.20376e-02, Loss_f: 1.90698e-02\n","Iter 170, Loss: 5.82707e-02, Loss_u: 4.27165e-02, Loss_f: 1.55542e-02\n","Iter 171, Loss: 5.72881e-02, Loss_u: 4.16388e-02, Loss_f: 1.56493e-02\n","Iter 172, Loss: 5.67737e-02, Loss_u: 4.13023e-02, Loss_f: 1.54713e-02\n","Iter 173, Loss: 5.65222e-02, Loss_u: 4.10143e-02, Loss_f: 1.55078e-02\n","Iter 174, Loss: 5.64082e-02, Loss_u: 4.08355e-02, Loss_f: 1.55726e-02\n","Iter 175, Loss: 5.63697e-02, Loss_u: 4.07447e-02, Loss_f: 1.56250e-02\n","Iter 176, Loss: 5.63288e-02, Loss_u: 4.11351e-02, Loss_f: 1.51937e-02\n","Iter 177, Loss: 5.62777e-02, Loss_u: 4.09884e-02, Loss_f: 1.52893e-02\n","Iter 178, Loss: 5.60808e-02, Loss_u: 4.03141e-02, Loss_f: 1.57667e-02\n","Iter 179, Loss: 5.57274e-02, Loss_u: 3.98382e-02, Loss_f: 1.58892e-02\n","Iter 180, Loss: 5.53402e-02, Loss_u: 3.93160e-02, Loss_f: 1.60242e-02\n","Iter 181, Loss: 5.49299e-02, Loss_u: 3.94720e-02, Loss_f: 1.54579e-02\n","Iter 182, Loss: 5.49299e-02, Loss_u: 3.94720e-02, Loss_f: 1.54579e-02\n","Iter 183, Loss: 5.46654e-02, Loss_u: 3.95259e-02, Loss_f: 1.51395e-02\n","Iter 184, Loss: 5.45160e-02, Loss_u: 3.91634e-02, Loss_f: 1.53525e-02\n","Iter 185, Loss: 5.43346e-02, Loss_u: 3.86567e-02, Loss_f: 1.56779e-02\n","Iter 186, Loss: 5.40922e-02, Loss_u: 3.81016e-02, Loss_f: 1.59907e-02\n","Iter 187, Loss: 5.37979e-02, Loss_u: 3.77795e-02, Loss_f: 1.60185e-02\n","Iter 188, Loss: 5.34308e-02, Loss_u: 3.79211e-02, Loss_f: 1.55097e-02\n","Iter 189, Loss: 5.31409e-02, Loss_u: 3.86312e-02, Loss_f: 1.45097e-02\n","Iter 190, Loss: 5.30161e-02, Loss_u: 3.88140e-02, Loss_f: 1.42021e-02\n","Iter 191, Loss: 5.29408e-02, Loss_u: 3.88459e-02, Loss_f: 1.40949e-02\n","Iter 192, Loss: 5.28502e-02, Loss_u: 3.84956e-02, Loss_f: 1.43546e-02\n","Iter 193, Loss: 5.27976e-02, Loss_u: 3.84951e-02, Loss_f: 1.43025e-02\n","Iter 194, Loss: 5.26765e-02, Loss_u: 3.83488e-02, Loss_f: 1.43276e-02\n","Iter 195, Loss: 5.23863e-02, Loss_u: 3.83470e-02, Loss_f: 1.40393e-02\n","Iter 196, Loss: 5.21248e-02, Loss_u: 3.82278e-02, Loss_f: 1.38970e-02\n","Iter 197, Loss: 5.16750e-02, Loss_u: 3.78874e-02, Loss_f: 1.37876e-02\n","Iter 198, Loss: 5.13962e-02, Loss_u: 3.73361e-02, Loss_f: 1.40601e-02\n","Iter 199, Loss: 5.09180e-02, Loss_u: 3.66444e-02, Loss_f: 1.42736e-02\n","Iter 200, Loss: 5.08149e-02, Loss_u: 3.74574e-02, Loss_f: 1.33575e-02\n","Iter 201, Loss: 5.07565e-02, Loss_u: 3.67011e-02, Loss_f: 1.40554e-02\n","Iter 202, Loss: 5.59091e-02, Loss_u: 3.67011e-02, Loss_f: 1.92080e-02\n","Iter 203, Loss: 5.63183e-02, Loss_u: 3.45407e-02, Loss_f: 2.17777e-02\n","Iter 204, Loss: 5.57545e-02, Loss_u: 3.58522e-02, Loss_f: 1.99023e-02\n","Iter 205, Loss: 5.48579e-02, Loss_u: 3.56613e-02, Loss_f: 1.91966e-02\n","Iter 206, Loss: 5.29854e-02, Loss_u: 3.59782e-02, Loss_f: 1.70072e-02\n","Iter 207, Loss: 5.26218e-02, Loss_u: 3.66693e-02, Loss_f: 1.59525e-02\n","Iter 208, Loss: 5.25003e-02, Loss_u: 3.74096e-02, Loss_f: 1.50907e-02\n","Iter 209, Loss: 5.22586e-02, Loss_u: 3.76065e-02, Loss_f: 1.46521e-02\n","Iter 210, Loss: 5.21068e-02, Loss_u: 3.71394e-02, Loss_f: 1.49674e-02\n","Iter 211, Loss: 5.20473e-02, Loss_u: 3.70716e-02, Loss_f: 1.49757e-02\n","Iter 212, Loss: 5.18963e-02, Loss_u: 3.68947e-02, Loss_f: 1.50016e-02\n","Iter 213, Loss: 5.17543e-02, Loss_u: 3.66565e-02, Loss_f: 1.50978e-02\n","Iter 214, Loss: 5.15420e-02, Loss_u: 3.59579e-02, Loss_f: 1.55841e-02\n","Iter 215, Loss: 5.11457e-02, Loss_u: 3.56100e-02, Loss_f: 1.55358e-02\n","Iter 216, Loss: 5.08593e-02, Loss_u: 3.47255e-02, Loss_f: 1.61338e-02\n","Iter 217, Loss: 5.03444e-02, Loss_u: 3.52577e-02, Loss_f: 1.50866e-02\n","Iter 218, Loss: 5.01065e-02, Loss_u: 3.61394e-02, Loss_f: 1.39671e-02\n","Iter 219, Loss: 4.98354e-02, Loss_u: 3.59936e-02, Loss_f: 1.38417e-02\n","Iter 220, Loss: 4.96115e-02, Loss_u: 3.58667e-02, Loss_f: 1.37447e-02\n","Iter 221, Loss: 4.93541e-02, Loss_u: 3.58923e-02, Loss_f: 1.34618e-02\n","Iter 222, Loss: 4.93541e-02, Loss_u: 3.58923e-02, Loss_f: 1.34618e-02\n","Iter 223, Loss: 4.89242e-02, Loss_u: 3.54718e-02, Loss_f: 1.34524e-02\n","Iter 224, Loss: 5.80255e-02, Loss_u: 3.54110e-02, Loss_f: 2.26145e-02\n","Iter 225, Loss: 4.87887e-02, Loss_u: 3.52684e-02, Loss_f: 1.35203e-02\n","Iter 226, Loss: 4.83060e-02, Loss_u: 3.52005e-02, Loss_f: 1.31055e-02\n","Iter 227, Loss: 4.77326e-02, Loss_u: 3.47352e-02, Loss_f: 1.29975e-02\n","Iter 228, Loss: 4.74148e-02, Loss_u: 3.44482e-02, Loss_f: 1.29666e-02\n","Iter 229, Loss: 4.73073e-02, Loss_u: 3.33317e-02, Loss_f: 1.39755e-02\n","Iter 230, Loss: 4.72468e-02, Loss_u: 3.43456e-02, Loss_f: 1.29012e-02\n","Iter 231, Loss: 4.69618e-02, Loss_u: 3.38171e-02, Loss_f: 1.31447e-02\n","Iter 232, Loss: 4.71674e-02, Loss_u: 3.39797e-02, Loss_f: 1.31877e-02\n","Iter 233, Loss: 4.66887e-02, Loss_u: 3.38058e-02, Loss_f: 1.28829e-02\n","Iter 234, Loss: 4.65769e-02, Loss_u: 3.37170e-02, Loss_f: 1.28599e-02\n","Iter 235, Loss: 4.62053e-02, Loss_u: 3.31313e-02, Loss_f: 1.30739e-02\n","Iter 236, Loss: 4.60096e-02, Loss_u: 3.28798e-02, Loss_f: 1.31298e-02\n","Iter 237, Loss: 4.58095e-02, Loss_u: 3.21869e-02, Loss_f: 1.36226e-02\n","Iter 238, Loss: 4.57258e-02, Loss_u: 3.22610e-02, Loss_f: 1.34648e-02\n","Iter 239, Loss: 4.56222e-02, Loss_u: 3.20577e-02, Loss_f: 1.35645e-02\n","Iter 240, Loss: 4.54454e-02, Loss_u: 3.19152e-02, Loss_f: 1.35302e-02\n","Iter 241, Loss: 4.52440e-02, Loss_u: 3.16769e-02, Loss_f: 1.35671e-02\n","Iter 242, Loss: 4.52440e-02, Loss_u: 3.16769e-02, Loss_f: 1.35671e-02\n","Iter 243, Loss: 4.49815e-02, Loss_u: 3.17408e-02, Loss_f: 1.32408e-02\n","Iter 244, Loss: 4.47320e-02, Loss_u: 3.15541e-02, Loss_f: 1.31779e-02\n","Iter 245, Loss: 4.44539e-02, Loss_u: 3.17113e-02, Loss_f: 1.27426e-02\n","Iter 246, Loss: 4.42712e-02, Loss_u: 3.11910e-02, Loss_f: 1.30802e-02\n","Iter 247, Loss: 4.40740e-02, Loss_u: 3.09534e-02, Loss_f: 1.31205e-02\n","Iter 248, Loss: 4.39553e-02, Loss_u: 3.06495e-02, Loss_f: 1.33058e-02\n","Iter 249, Loss: 4.37363e-02, Loss_u: 3.06543e-02, Loss_f: 1.30820e-02\n","Iter 250, Loss: 4.35373e-02, Loss_u: 3.03749e-02, Loss_f: 1.31624e-02\n","Iter 251, Loss: 4.32842e-02, Loss_u: 3.01702e-02, Loss_f: 1.31140e-02\n","Iter 252, Loss: 4.27943e-02, Loss_u: 3.03902e-02, Loss_f: 1.24040e-02\n","Iter 253, Loss: 4.24932e-02, Loss_u: 3.03387e-02, Loss_f: 1.21545e-02\n","Iter 254, Loss: 4.23245e-02, Loss_u: 3.01321e-02, Loss_f: 1.21923e-02\n","Iter 255, Loss: 4.23140e-02, Loss_u: 2.98969e-02, Loss_f: 1.24171e-02\n","Iter 256, Loss: 4.22671e-02, Loss_u: 3.00014e-02, Loss_f: 1.22657e-02\n","Iter 257, Loss: 4.21843e-02, Loss_u: 2.94292e-02, Loss_f: 1.27552e-02\n","Iter 258, Loss: 4.21202e-02, Loss_u: 2.94966e-02, Loss_f: 1.26235e-02\n","Iter 259, Loss: 4.19600e-02, Loss_u: 2.98358e-02, Loss_f: 1.21242e-02\n","Iter 260, Loss: 4.16504e-02, Loss_u: 2.96792e-02, Loss_f: 1.19712e-02\n","Iter 261, Loss: 4.12672e-02, Loss_u: 2.95336e-02, Loss_f: 1.17336e-02\n","Iter 262, Loss: 4.12672e-02, Loss_u: 2.95336e-02, Loss_f: 1.17336e-02\n","Iter 263, Loss: 4.10018e-02, Loss_u: 2.92604e-02, Loss_f: 1.17414e-02\n","Iter 264, Loss: 4.07706e-02, Loss_u: 2.91027e-02, Loss_f: 1.16679e-02\n","Iter 265, Loss: 4.06087e-02, Loss_u: 2.99380e-02, Loss_f: 1.06708e-02\n","Iter 266, Loss: 4.03512e-02, Loss_u: 2.93547e-02, Loss_f: 1.09966e-02\n","Iter 267, Loss: 4.01904e-02, Loss_u: 2.90032e-02, Loss_f: 1.11872e-02\n","Iter 268, Loss: 4.01088e-02, Loss_u: 2.92443e-02, Loss_f: 1.08645e-02\n","Iter 269, Loss: 4.00273e-02, Loss_u: 2.95197e-02, Loss_f: 1.05076e-02\n","Iter 270, Loss: 3.99963e-02, Loss_u: 2.94599e-02, Loss_f: 1.05364e-02\n","Iter 271, Loss: 3.99532e-02, Loss_u: 2.93828e-02, Loss_f: 1.05704e-02\n","Iter 272, Loss: 3.99073e-02, Loss_u: 2.91052e-02, Loss_f: 1.08021e-02\n","Iter 273, Loss: 3.98527e-02, Loss_u: 2.87519e-02, Loss_f: 1.11009e-02\n","Iter 274, Loss: 3.97823e-02, Loss_u: 2.83948e-02, Loss_f: 1.13875e-02\n","Iter 275, Loss: 3.96763e-02, Loss_u: 2.81423e-02, Loss_f: 1.15341e-02\n","Iter 276, Loss: 3.95257e-02, Loss_u: 2.79577e-02, Loss_f: 1.15679e-02\n","Iter 277, Loss: 3.94063e-02, Loss_u: 2.81576e-02, Loss_f: 1.12487e-02\n","Iter 278, Loss: 3.93033e-02, Loss_u: 2.83749e-02, Loss_f: 1.09284e-02\n","Iter 279, Loss: 3.92307e-02, Loss_u: 2.85687e-02, Loss_f: 1.06619e-02\n","Iter 280, Loss: 3.90456e-02, Loss_u: 2.88156e-02, Loss_f: 1.02299e-02\n","Iter 281, Loss: 3.87345e-02, Loss_u: 2.85621e-02, Loss_f: 1.01724e-02\n","Iter 282, Loss: 3.87345e-02, Loss_u: 2.85621e-02, Loss_f: 1.01724e-02\n","Iter 283, Loss: 3.84362e-02, Loss_u: 2.80925e-02, Loss_f: 1.03437e-02\n","Iter 284, Loss: 3.81149e-02, Loss_u: 2.75401e-02, Loss_f: 1.05748e-02\n","Iter 285, Loss: 3.78337e-02, Loss_u: 2.67074e-02, Loss_f: 1.11263e-02\n","Iter 286, Loss: 3.78955e-02, Loss_u: 2.71528e-02, Loss_f: 1.07427e-02\n","Iter 287, Loss: 3.77148e-02, Loss_u: 2.68943e-02, Loss_f: 1.08205e-02\n","Iter 288, Loss: 3.75840e-02, Loss_u: 2.69105e-02, Loss_f: 1.06736e-02\n","Iter 289, Loss: 3.74349e-02, Loss_u: 2.72916e-02, Loss_f: 1.01432e-02\n","Iter 290, Loss: 3.73730e-02, Loss_u: 2.68111e-02, Loss_f: 1.05619e-02\n","Iter 291, Loss: 3.73002e-02, Loss_u: 2.70291e-02, Loss_f: 1.02711e-02\n","Iter 292, Loss: 3.72152e-02, Loss_u: 2.69247e-02, Loss_f: 1.02904e-02\n","Iter 293, Loss: 3.71534e-02, Loss_u: 2.67362e-02, Loss_f: 1.04171e-02\n","Iter 294, Loss: 3.71115e-02, Loss_u: 2.64554e-02, Loss_f: 1.06561e-02\n","Iter 295, Loss: 3.70057e-02, Loss_u: 2.63425e-02, Loss_f: 1.06632e-02\n","Iter 296, Loss: 3.71377e-02, Loss_u: 2.62760e-02, Loss_f: 1.08617e-02\n","Iter 297, Loss: 3.69245e-02, Loss_u: 2.63026e-02, Loss_f: 1.06219e-02\n","Iter 298, Loss: 3.67687e-02, Loss_u: 2.63317e-02, Loss_f: 1.04370e-02\n","Iter 299, Loss: 3.66188e-02, Loss_u: 2.62881e-02, Loss_f: 1.03306e-02\n","Iter 300, Loss: 3.64837e-02, Loss_u: 2.63975e-02, Loss_f: 1.00861e-02\n","Iter 301, Loss: 3.63732e-02, Loss_u: 2.64458e-02, Loss_f: 9.92741e-03\n","Iter 302, Loss: 3.63732e-02, Loss_u: 2.64458e-02, Loss_f: 9.92741e-03\n","Iter 303, Loss: 3.62674e-02, Loss_u: 2.62123e-02, Loss_f: 1.00551e-02\n","Iter 304, Loss: 3.61903e-02, Loss_u: 2.59802e-02, Loss_f: 1.02101e-02\n","Iter 305, Loss: 3.61315e-02, Loss_u: 2.61027e-02, Loss_f: 1.00288e-02\n","Iter 306, Loss: 3.60796e-02, Loss_u: 2.63377e-02, Loss_f: 9.74192e-03\n","Iter 307, Loss: 3.60462e-02, Loss_u: 2.63799e-02, Loss_f: 9.66632e-03\n","Iter 308, Loss: 3.60112e-02, Loss_u: 2.64563e-02, Loss_f: 9.55484e-03\n","Iter 309, Loss: 3.59417e-02, Loss_u: 2.66534e-02, Loss_f: 9.28833e-03\n","Iter 310, Loss: 3.57836e-02, Loss_u: 2.67526e-02, Loss_f: 9.03105e-03\n","Iter 311, Loss: 3.56163e-02, Loss_u: 2.66841e-02, Loss_f: 8.93222e-03\n","Iter 312, Loss: 3.56013e-02, Loss_u: 2.60911e-02, Loss_f: 9.51017e-03\n","Iter 313, Loss: 3.55164e-02, Loss_u: 2.63624e-02, Loss_f: 9.15401e-03\n","Iter 314, Loss: 3.54912e-02, Loss_u: 2.59413e-02, Loss_f: 9.54991e-03\n","Iter 315, Loss: 3.54066e-02, Loss_u: 2.61079e-02, Loss_f: 9.29869e-03\n","Iter 316, Loss: 3.53492e-02, Loss_u: 2.58530e-02, Loss_f: 9.49616e-03\n","Iter 317, Loss: 3.53304e-02, Loss_u: 2.57280e-02, Loss_f: 9.60236e-03\n","Iter 318, Loss: 3.53116e-02, Loss_u: 2.58055e-02, Loss_f: 9.50606e-03\n","Iter 319, Loss: 3.52780e-02, Loss_u: 2.58419e-02, Loss_f: 9.43613e-03\n","Iter 320, Loss: 3.52340e-02, Loss_u: 2.59418e-02, Loss_f: 9.29217e-03\n","Iter 321, Loss: 3.53906e-02, Loss_u: 2.62938e-02, Loss_f: 9.09678e-03\n","Iter 322, Loss: 3.52160e-02, Loss_u: 2.60139e-02, Loss_f: 9.20216e-03\n","Iter 323, Loss: 3.52160e-02, Loss_u: 2.60139e-02, Loss_f: 9.20216e-03\n","Iter 324, Loss: 3.51555e-02, Loss_u: 2.60632e-02, Loss_f: 9.09232e-03\n","Iter 325, Loss: 3.51235e-02, Loss_u: 2.58212e-02, Loss_f: 9.30222e-03\n","Iter 326, Loss: 3.50758e-02, Loss_u: 2.57523e-02, Loss_f: 9.32353e-03\n","Iter 327, Loss: 3.51071e-02, Loss_u: 2.55572e-02, Loss_f: 9.54990e-03\n","Iter 328, Loss: 3.50611e-02, Loss_u: 2.56720e-02, Loss_f: 9.38908e-03\n","Iter 329, Loss: 3.50124e-02, Loss_u: 2.55161e-02, Loss_f: 9.49631e-03\n","Iter 330, Loss: 3.49619e-02, Loss_u: 2.54149e-02, Loss_f: 9.54698e-03\n","Iter 331, Loss: 3.48016e-02, Loss_u: 2.50663e-02, Loss_f: 9.73533e-03\n","Iter 332, Loss: 3.47087e-02, Loss_u: 2.50311e-02, Loss_f: 9.67764e-03\n","Iter 333, Loss: 3.56129e-02, Loss_u: 2.59592e-02, Loss_f: 9.65365e-03\n","Iter 334, Loss: 3.45386e-02, Loss_u: 2.53840e-02, Loss_f: 9.15453e-03\n","Iter 335, Loss: 3.43682e-02, Loss_u: 2.51756e-02, Loss_f: 9.19265e-03\n","Iter 336, Loss: 3.41700e-02, Loss_u: 2.51111e-02, Loss_f: 9.05893e-03\n","Iter 337, Loss: 3.42683e-02, Loss_u: 2.54990e-02, Loss_f: 8.76931e-03\n","Iter 338, Loss: 3.40511e-02, Loss_u: 2.52417e-02, Loss_f: 8.80941e-03\n","Iter 339, Loss: 3.40158e-02, Loss_u: 2.49903e-02, Loss_f: 9.02547e-03\n","Iter 340, Loss: 3.39427e-02, Loss_u: 2.51014e-02, Loss_f: 8.84133e-03\n","Iter 341, Loss: 3.38833e-02, Loss_u: 2.51047e-02, Loss_f: 8.77866e-03\n","Iter 342, Loss: 3.38339e-02, Loss_u: 2.53454e-02, Loss_f: 8.48852e-03\n","Iter 343, Loss: 3.38339e-02, Loss_u: 2.53454e-02, Loss_f: 8.48852e-03\n","Iter 344, Loss: 3.37900e-02, Loss_u: 2.53815e-02, Loss_f: 8.40851e-03\n","Iter 345, Loss: 3.37125e-02, Loss_u: 2.53440e-02, Loss_f: 8.36847e-03\n","Iter 346, Loss: 3.36793e-02, Loss_u: 2.52089e-02, Loss_f: 8.47043e-03\n","Iter 347, Loss: 3.36372e-02, Loss_u: 2.51250e-02, Loss_f: 8.51223e-03\n","Iter 348, Loss: 3.36176e-02, Loss_u: 2.49769e-02, Loss_f: 8.64062e-03\n","Iter 349, Loss: 3.35972e-02, Loss_u: 2.49506e-02, Loss_f: 8.64655e-03\n","Iter 350, Loss: 3.35783e-02, Loss_u: 2.48018e-02, Loss_f: 8.77653e-03\n","Iter 351, Loss: 3.35606e-02, Loss_u: 2.47319e-02, Loss_f: 8.82870e-03\n","Iter 352, Loss: 3.35413e-02, Loss_u: 2.47061e-02, Loss_f: 8.83521e-03\n","Iter 353, Loss: 3.35031e-02, Loss_u: 2.46782e-02, Loss_f: 8.82495e-03\n","Iter 354, Loss: 3.35113e-02, Loss_u: 2.46203e-02, Loss_f: 8.89101e-03\n","Iter 355, Loss: 3.34693e-02, Loss_u: 2.46363e-02, Loss_f: 8.83299e-03\n","Iter 356, Loss: 3.33978e-02, Loss_u: 2.46766e-02, Loss_f: 8.72125e-03\n","Iter 357, Loss: 3.33316e-02, Loss_u: 2.45685e-02, Loss_f: 8.76310e-03\n","Iter 358, Loss: 3.32991e-02, Loss_u: 2.46151e-02, Loss_f: 8.68400e-03\n","Iter 359, Loss: 3.32382e-02, Loss_u: 2.45200e-02, Loss_f: 8.71820e-03\n","Iter 360, Loss: 3.31985e-02, Loss_u: 2.44611e-02, Loss_f: 8.73740e-03\n","Iter 361, Loss: 3.31295e-02, Loss_u: 2.43796e-02, Loss_f: 8.74984e-03\n","Iter 362, Loss: 3.30582e-02, Loss_u: 2.41612e-02, Loss_f: 8.89702e-03\n","Iter 363, Loss: 3.30582e-02, Loss_u: 2.41612e-02, Loss_f: 8.89702e-03\n","Iter 364, Loss: 3.29200e-02, Loss_u: 2.41422e-02, Loss_f: 8.77786e-03\n","Iter 365, Loss: 3.27703e-02, Loss_u: 2.40107e-02, Loss_f: 8.75956e-03\n","Iter 366, Loss: 3.25605e-02, Loss_u: 2.37787e-02, Loss_f: 8.78176e-03\n","Iter 367, Loss: 3.24394e-02, Loss_u: 2.33545e-02, Loss_f: 9.08494e-03\n","Iter 368, Loss: 3.23476e-02, Loss_u: 2.29942e-02, Loss_f: 9.35338e-03\n","Iter 369, Loss: 3.22710e-02, Loss_u: 2.30047e-02, Loss_f: 9.26636e-03\n","Iter 370, Loss: 3.22184e-02, Loss_u: 2.30039e-02, Loss_f: 9.21452e-03\n","Iter 371, Loss: 3.21557e-02, Loss_u: 2.28716e-02, Loss_f: 9.28406e-03\n","Iter 372, Loss: 3.21086e-02, Loss_u: 2.27515e-02, Loss_f: 9.35712e-03\n","Iter 373, Loss: 3.20719e-02, Loss_u: 2.27649e-02, Loss_f: 9.30697e-03\n","Iter 374, Loss: 3.20387e-02, Loss_u: 2.26800e-02, Loss_f: 9.35868e-03\n","Iter 375, Loss: 3.20057e-02, Loss_u: 2.26006e-02, Loss_f: 9.40509e-03\n","Iter 376, Loss: 3.19560e-02, Loss_u: 2.28450e-02, Loss_f: 9.11109e-03\n","Iter 377, Loss: 3.18683e-02, Loss_u: 2.29187e-02, Loss_f: 8.94962e-03\n","Iter 378, Loss: 3.17676e-02, Loss_u: 2.26731e-02, Loss_f: 9.09452e-03\n","Iter 379, Loss: 3.16648e-02, Loss_u: 2.25273e-02, Loss_f: 9.13750e-03\n","Iter 380, Loss: 3.45810e-02, Loss_u: 2.21249e-02, Loss_f: 1.24560e-02\n","Iter 381, Loss: 3.16245e-02, Loss_u: 2.24453e-02, Loss_f: 9.17919e-03\n","Iter 382, Loss: 3.15338e-02, Loss_u: 2.23907e-02, Loss_f: 9.14312e-03\n","Iter 383, Loss: 3.15338e-02, Loss_u: 2.23907e-02, Loss_f: 9.14312e-03\n","Iter 384, Loss: 3.14252e-02, Loss_u: 2.21308e-02, Loss_f: 9.29441e-03\n","Iter 385, Loss: 3.13092e-02, Loss_u: 2.20076e-02, Loss_f: 9.30155e-03\n","Iter 386, Loss: 3.11797e-02, Loss_u: 2.15091e-02, Loss_f: 9.67059e-03\n","Iter 387, Loss: 3.11636e-02, Loss_u: 2.10798e-02, Loss_f: 1.00838e-02\n","Iter 388, Loss: 3.10684e-02, Loss_u: 2.12371e-02, Loss_f: 9.83135e-03\n","Iter 389, Loss: 3.09077e-02, Loss_u: 2.07789e-02, Loss_f: 1.01288e-02\n","Iter 390, Loss: 3.06971e-02, Loss_u: 2.01148e-02, Loss_f: 1.05823e-02\n","Iter 391, Loss: 3.10261e-02, Loss_u: 1.99605e-02, Loss_f: 1.10657e-02\n","Iter 392, Loss: 3.05245e-02, Loss_u: 1.99556e-02, Loss_f: 1.05689e-02\n","Iter 393, Loss: 3.03602e-02, Loss_u: 2.00094e-02, Loss_f: 1.03508e-02\n","Iter 394, Loss: 3.02915e-02, Loss_u: 2.02327e-02, Loss_f: 1.00588e-02\n","Iter 395, Loss: 3.05118e-02, Loss_u: 2.04176e-02, Loss_f: 1.00942e-02\n","Iter 396, Loss: 3.01518e-02, Loss_u: 2.02341e-02, Loss_f: 9.91771e-03\n","Iter 397, Loss: 3.00722e-02, Loss_u: 1.98891e-02, Loss_f: 1.01830e-02\n","Iter 398, Loss: 2.99083e-02, Loss_u: 1.98570e-02, Loss_f: 1.00512e-02\n","Iter 399, Loss: 2.97163e-02, Loss_u: 1.95280e-02, Loss_f: 1.01883e-02\n","Iter 400, Loss: 2.96030e-02, Loss_u: 1.85962e-02, Loss_f: 1.10068e-02\n","Iter 401, Loss: 2.93335e-02, Loss_u: 1.88630e-02, Loss_f: 1.04705e-02\n","Iter 402, Loss: 2.91577e-02, Loss_u: 1.83848e-02, Loss_f: 1.07729e-02\n","Iter 403, Loss: 1.78156e-01, Loss_u: 1.83848e-02, Loss_f: 1.59771e-01\n","Iter 404, Loss: 1.50324e+00, Loss_u: 1.84843e-02, Loss_f: 1.48475e+00\n","Iter 405, Loss: 1.78085e-01, Loss_u: 1.83740e-02, Loss_f: 1.59711e-01\n","Iter 406, Loss: 2.54470e+00, Loss_u: 7.36825e-02, Loss_f: 2.47101e+00\n","Iter 407, Loss: 1.77684e-01, Loss_u: 1.83481e-02, Loss_f: 1.59336e-01\n","Iter 408, Loss: 1.29887e+01, Loss_u: 3.01656e-01, Loss_f: 1.26871e+01\n","Iter 409, Loss: 1.75557e-01, Loss_u: 1.82604e-02, Loss_f: 1.57296e-01\n","Iter 410, Loss: 1.60865e-01, Loss_u: 1.77933e-02, Loss_f: 1.43071e-01\n","Iter 411, Loss: 1.56278e-01, Loss_u: 1.77608e-02, Loss_f: 1.38517e-01\n","Iter 412, Loss: 2.05898e+01, Loss_u: 3.06199e-01, Loss_f: 2.02836e+01\n","Iter 413, Loss: 4.64879e+00, Loss_u: 6.03639e-02, Loss_f: 4.58843e+00\n","Iter 414, Loss: 1.52740e-01, Loss_u: 1.78569e-02, Loss_f: 1.34883e-01\n","Iter 415, Loss: 1.42171e-01, Loss_u: 1.86694e-02, Loss_f: 1.23502e-01\n","Iter 416, Loss: 1.28845e+01, Loss_u: 4.17825e-01, Loss_f: 1.24667e+01\n","Iter 417, Loss: 1.34405e-01, Loss_u: 1.92059e-02, Loss_f: 1.15199e-01\n","Iter 418, Loss: 1.57093e+01, Loss_u: 3.82558e-01, Loss_f: 1.53268e+01\n","Iter 419, Loss: 1.26665e-01, Loss_u: 1.94775e-02, Loss_f: 1.07188e-01\n","Iter 420, Loss: 1.08053e-01, Loss_u: 2.38962e-02, Loss_f: 8.41568e-02\n","Iter 421, Loss: 9.21241e-02, Loss_u: 2.21016e-02, Loss_f: 7.00224e-02\n","Iter 422, Loss: 1.20066e-01, Loss_u: 4.04666e-02, Loss_f: 7.95993e-02\n","Iter 423, Loss: 6.45902e-02, Loss_u: 2.50195e-02, Loss_f: 3.95707e-02\n","Iter 424, Loss: 6.45902e-02, Loss_u: 2.50195e-02, Loss_f: 3.95707e-02\n","Iter 425, Loss: 5.87713e-01, Loss_u: 5.65213e-02, Loss_f: 5.31192e-01\n","Iter 426, Loss: 6.01700e-02, Loss_u: 2.56880e-02, Loss_f: 3.44820e-02\n","Iter 427, Loss: 6.02832e-02, Loss_u: 2.79562e-02, Loss_f: 3.23269e-02\n","Iter 428, Loss: 5.32604e-02, Loss_u: 2.65070e-02, Loss_f: 2.67534e-02\n","Iter 429, Loss: 4.90611e-02, Loss_u: 3.12094e-02, Loss_f: 1.78517e-02\n","Iter 430, Loss: 4.31916e-02, Loss_u: 3.14843e-02, Loss_f: 1.17073e-02\n","Iter 431, Loss: 4.24828e-02, Loss_u: 3.05283e-02, Loss_f: 1.19545e-02\n","Iter 432, Loss: 4.14402e-02, Loss_u: 2.98102e-02, Loss_f: 1.16300e-02\n","Iter 433, Loss: 4.04351e-02, Loss_u: 2.91066e-02, Loss_f: 1.13285e-02\n","Iter 434, Loss: 4.06223e-02, Loss_u: 2.64028e-02, Loss_f: 1.42195e-02\n","Iter 435, Loss: 3.94168e-02, Loss_u: 2.75872e-02, Loss_f: 1.18297e-02\n","Iter 436, Loss: 3.85252e-02, Loss_u: 2.70208e-02, Loss_f: 1.15044e-02\n","Iter 437, Loss: 3.76705e-02, Loss_u: 2.57500e-02, Loss_f: 1.19205e-02\n","Iter 438, Loss: 3.74185e-02, Loss_u: 2.53311e-02, Loss_f: 1.20873e-02\n","Iter 439, Loss: 3.75076e-02, Loss_u: 2.56885e-02, Loss_f: 1.18191e-02\n","Iter 440, Loss: 3.72253e-02, Loss_u: 2.54765e-02, Loss_f: 1.17488e-02\n","Iter 441, Loss: 3.68758e-02, Loss_u: 2.50323e-02, Loss_f: 1.18435e-02\n","Iter 442, Loss: 3.64897e-02, Loss_u: 2.44812e-02, Loss_f: 1.20085e-02\n","Iter 443, Loss: 3.63291e-02, Loss_u: 2.24839e-02, Loss_f: 1.38452e-02\n","Iter 444, Loss: 3.56947e-02, Loss_u: 2.31931e-02, Loss_f: 1.25017e-02\n","Iter 445, Loss: 3.56947e-02, Loss_u: 2.31931e-02, Loss_f: 1.25017e-02\n","Iter 446, Loss: 3.49458e-02, Loss_u: 2.29124e-02, Loss_f: 1.20334e-02\n","Iter 447, Loss: 3.44016e-02, Loss_u: 2.30051e-02, Loss_f: 1.13966e-02\n","Iter 448, Loss: 3.40223e-02, Loss_u: 2.25731e-02, Loss_f: 1.14492e-02\n","Iter 449, Loss: 3.36390e-02, Loss_u: 2.19693e-02, Loss_f: 1.16696e-02\n","Iter 450, Loss: 3.33553e-02, Loss_u: 2.11310e-02, Loss_f: 1.22243e-02\n","Iter 451, Loss: 3.29225e-02, Loss_u: 2.06787e-02, Loss_f: 1.22437e-02\n","Iter 452, Loss: 3.22789e-02, Loss_u: 2.01936e-02, Loss_f: 1.20853e-02\n","Iter 453, Loss: 3.14491e-02, Loss_u: 1.94991e-02, Loss_f: 1.19500e-02\n","Iter 454, Loss: 3.10832e-02, Loss_u: 1.94556e-02, Loss_f: 1.16275e-02\n","Iter 455, Loss: 3.07554e-02, Loss_u: 1.93731e-02, Loss_f: 1.13823e-02\n","Iter 456, Loss: 3.04441e-02, Loss_u: 1.90828e-02, Loss_f: 1.13613e-02\n","Iter 457, Loss: 3.00472e-02, Loss_u: 1.94898e-02, Loss_f: 1.05574e-02\n","Iter 458, Loss: 2.96800e-02, Loss_u: 1.96224e-02, Loss_f: 1.00577e-02\n","Iter 459, Loss: 2.94161e-02, Loss_u: 1.96246e-02, Loss_f: 9.79154e-03\n","Iter 460, Loss: 2.91339e-02, Loss_u: 1.92898e-02, Loss_f: 9.84411e-03\n","Iter 461, Loss: 2.88101e-02, Loss_u: 1.88194e-02, Loss_f: 9.99073e-03\n","Iter 462, Loss: 2.83805e-02, Loss_u: 1.78034e-02, Loss_f: 1.05770e-02\n","Iter 463, Loss: 2.79163e-02, Loss_u: 1.73203e-02, Loss_f: 1.05960e-02\n","Iter 464, Loss: 2.76580e-02, Loss_u: 1.65810e-02, Loss_f: 1.10770e-02\n","Iter 465, Loss: 2.76580e-02, Loss_u: 1.65810e-02, Loss_f: 1.10770e-02\n","Iter 466, Loss: 2.74931e-02, Loss_u: 1.67849e-02, Loss_f: 1.07082e-02\n","Iter 467, Loss: 2.72886e-02, Loss_u: 1.67700e-02, Loss_f: 1.05186e-02\n","Iter 468, Loss: 2.69559e-02, Loss_u: 1.68207e-02, Loss_f: 1.01352e-02\n","Iter 469, Loss: 2.66049e-02, Loss_u: 1.61053e-02, Loss_f: 1.04996e-02\n","Iter 470, Loss: 2.63096e-02, Loss_u: 1.61027e-02, Loss_f: 1.02069e-02\n","Iter 471, Loss: 2.61362e-02, Loss_u: 1.58043e-02, Loss_f: 1.03319e-02\n","Iter 472, Loss: 2.59970e-02, Loss_u: 1.54625e-02, Loss_f: 1.05345e-02\n","Iter 473, Loss: 2.58722e-02, Loss_u: 1.53775e-02, Loss_f: 1.04947e-02\n","Iter 474, Loss: 2.57264e-02, Loss_u: 1.54613e-02, Loss_f: 1.02652e-02\n","Iter 475, Loss: 2.58326e-02, Loss_u: 1.53841e-02, Loss_f: 1.04486e-02\n","Iter 476, Loss: 2.56820e-02, Loss_u: 1.54290e-02, Loss_f: 1.02529e-02\n","Iter 477, Loss: 2.55240e-02, Loss_u: 1.53787e-02, Loss_f: 1.01453e-02\n","Iter 478, Loss: 2.72068e-02, Loss_u: 1.70357e-02, Loss_f: 1.01711e-02\n","Iter 479, Loss: 2.55041e-02, Loss_u: 1.54742e-02, Loss_f: 1.00298e-02\n","Iter 480, Loss: 2.53804e-02, Loss_u: 1.55226e-02, Loss_f: 9.85775e-03\n","Iter 481, Loss: 2.52832e-02, Loss_u: 1.53144e-02, Loss_f: 9.96882e-03\n","Iter 482, Loss: 3.51309e-02, Loss_u: 1.90043e-02, Loss_f: 1.61266e-02\n","Iter 483, Loss: 2.52098e-02, Loss_u: 1.54209e-02, Loss_f: 9.78889e-03\n","Iter 484, Loss: 2.50964e-02, Loss_u: 1.52656e-02, Loss_f: 9.83079e-03\n","Iter 485, Loss: 2.50964e-02, Loss_u: 1.52656e-02, Loss_f: 9.83079e-03\n","Iter 486, Loss: 3.33606e-02, Loss_u: 1.58532e-02, Loss_f: 1.75074e-02\n","Iter 487, Loss: 2.50333e-02, Loss_u: 1.52223e-02, Loss_f: 9.81092e-03\n","Iter 488, Loss: 2.48323e-02, Loss_u: 1.51091e-02, Loss_f: 9.72322e-03\n","Iter 489, Loss: 3.81512e-02, Loss_u: 1.23450e-02, Loss_f: 2.58062e-02\n","Iter 490, Loss: 2.47894e-02, Loss_u: 1.47959e-02, Loss_f: 9.99350e-03\n","Iter 491, Loss: 2.45281e-02, Loss_u: 1.48817e-02, Loss_f: 9.64635e-03\n","Iter 492, Loss: 2.45559e-02, Loss_u: 1.50198e-02, Loss_f: 9.53613e-03\n","Iter 493, Loss: 2.42508e-02, Loss_u: 1.49496e-02, Loss_f: 9.30117e-03\n","Iter 494, Loss: 2.40786e-02, Loss_u: 1.49488e-02, Loss_f: 9.12974e-03\n","Iter 495, Loss: 2.39139e-02, Loss_u: 1.46876e-02, Loss_f: 9.22625e-03\n","Iter 496, Loss: 2.38680e-02, Loss_u: 1.50531e-02, Loss_f: 8.81487e-03\n","Iter 497, Loss: 2.38129e-02, Loss_u: 1.49313e-02, Loss_f: 8.88162e-03\n","Iter 498, Loss: 2.37492e-02, Loss_u: 1.47563e-02, Loss_f: 8.99290e-03\n","Iter 499, Loss: 2.36630e-02, Loss_u: 1.46494e-02, Loss_f: 9.01363e-03\n","Iter 500, Loss: 2.34962e-02, Loss_u: 1.45338e-02, Loss_f: 8.96242e-03\n","Iter 501, Loss: 2.33313e-02, Loss_u: 1.43920e-02, Loss_f: 8.93934e-03\n","Iter 502, Loss: 2.31472e-02, Loss_u: 1.43443e-02, Loss_f: 8.80292e-03\n","Iter 503, Loss: 2.30477e-02, Loss_u: 1.41611e-02, Loss_f: 8.88659e-03\n","Iter 504, Loss: 2.29380e-02, Loss_u: 1.46584e-02, Loss_f: 8.27956e-03\n","Iter 505, Loss: 2.29380e-02, Loss_u: 1.46584e-02, Loss_f: 8.27956e-03\n","Iter 506, Loss: 2.27748e-02, Loss_u: 1.43812e-02, Loss_f: 8.39356e-03\n","Iter 507, Loss: 2.27215e-02, Loss_u: 1.41460e-02, Loss_f: 8.57551e-03\n","Iter 508, Loss: 2.26839e-02, Loss_u: 1.41560e-02, Loss_f: 8.52781e-03\n","Iter 509, Loss: 2.25631e-02, Loss_u: 1.38072e-02, Loss_f: 8.75591e-03\n","Iter 510, Loss: 2.24707e-02, Loss_u: 1.38176e-02, Loss_f: 8.65312e-03\n","Iter 511, Loss: 2.24226e-02, Loss_u: 1.38986e-02, Loss_f: 8.52405e-03\n","Iter 512, Loss: 2.23745e-02, Loss_u: 1.38650e-02, Loss_f: 8.50949e-03\n","Iter 513, Loss: 2.23244e-02, Loss_u: 1.40251e-02, Loss_f: 8.29929e-03\n","Iter 514, Loss: 2.22522e-02, Loss_u: 1.39764e-02, Loss_f: 8.27581e-03\n","Iter 515, Loss: 2.21820e-02, Loss_u: 1.38674e-02, Loss_f: 8.31461e-03\n","Iter 516, Loss: 2.21111e-02, Loss_u: 1.38111e-02, Loss_f: 8.30007e-03\n","Iter 517, Loss: 2.20572e-02, Loss_u: 1.38378e-02, Loss_f: 8.21939e-03\n","Iter 518, Loss: 2.20192e-02, Loss_u: 1.39005e-02, Loss_f: 8.11870e-03\n","Iter 519, Loss: 2.19944e-02, Loss_u: 1.39345e-02, Loss_f: 8.05991e-03\n","Iter 520, Loss: 2.23662e-02, Loss_u: 1.38336e-02, Loss_f: 8.53256e-03\n","Iter 521, Loss: 2.19748e-02, Loss_u: 1.38991e-02, Loss_f: 8.07569e-03\n","Iter 522, Loss: 2.18952e-02, Loss_u: 1.39754e-02, Loss_f: 7.91979e-03\n","Iter 523, Loss: 2.17642e-02, Loss_u: 1.41852e-02, Loss_f: 7.57903e-03\n","Iter 524, Loss: 2.16172e-02, Loss_u: 1.41271e-02, Loss_f: 7.49013e-03\n","Iter 525, Loss: 2.16172e-02, Loss_u: 1.41271e-02, Loss_f: 7.49013e-03\n","Iter 526, Loss: 2.14368e-02, Loss_u: 1.40470e-02, Loss_f: 7.38974e-03\n","Iter 527, Loss: 2.12632e-02, Loss_u: 1.38840e-02, Loss_f: 7.37917e-03\n","Iter 528, Loss: 2.11724e-02, Loss_u: 1.39683e-02, Loss_f: 7.20407e-03\n","Iter 529, Loss: 2.10885e-02, Loss_u: 1.38570e-02, Loss_f: 7.23153e-03\n","Iter 530, Loss: 2.10363e-02, Loss_u: 1.40207e-02, Loss_f: 7.01561e-03\n","Iter 531, Loss: 2.09922e-02, Loss_u: 1.40552e-02, Loss_f: 6.93697e-03\n","Iter 532, Loss: 2.09523e-02, Loss_u: 1.39980e-02, Loss_f: 6.95430e-03\n","Iter 533, Loss: 2.09162e-02, Loss_u: 1.39758e-02, Loss_f: 6.94040e-03\n","Iter 534, Loss: 2.08917e-02, Loss_u: 1.39087e-02, Loss_f: 6.98298e-03\n","Iter 535, Loss: 2.08746e-02, Loss_u: 1.39065e-02, Loss_f: 6.96817e-03\n","Iter 536, Loss: 2.08651e-02, Loss_u: 1.38888e-02, Loss_f: 6.97636e-03\n","Iter 537, Loss: 2.08536e-02, Loss_u: 1.38995e-02, Loss_f: 6.95403e-03\n","Iter 538, Loss: 2.08131e-02, Loss_u: 1.39116e-02, Loss_f: 6.90147e-03\n","Iter 539, Loss: 2.07659e-02, Loss_u: 1.37910e-02, Loss_f: 6.97489e-03\n","Iter 540, Loss: 2.06938e-02, Loss_u: 1.37904e-02, Loss_f: 6.90341e-03\n","Iter 541, Loss: 2.05732e-02, Loss_u: 1.38046e-02, Loss_f: 6.76863e-03\n","Iter 542, Loss: 2.02833e-02, Loss_u: 1.36460e-02, Loss_f: 6.63733e-03\n","Iter 543, Loss: 2.01465e-02, Loss_u: 1.34536e-02, Loss_f: 6.69293e-03\n","Iter 544, Loss: 2.00387e-02, Loss_u: 1.33892e-02, Loss_f: 6.64949e-03\n","Iter 545, Loss: 2.00387e-02, Loss_u: 1.33892e-02, Loss_f: 6.64949e-03\n","Iter 546, Loss: 1.99134e-02, Loss_u: 1.34219e-02, Loss_f: 6.49147e-03\n","Iter 547, Loss: 1.99013e-02, Loss_u: 1.36199e-02, Loss_f: 6.28142e-03\n","Iter 548, Loss: 1.98149e-02, Loss_u: 1.35291e-02, Loss_f: 6.28586e-03\n","Iter 549, Loss: 1.97681e-02, Loss_u: 1.35661e-02, Loss_f: 6.20195e-03\n","Iter 550, Loss: 1.97169e-02, Loss_u: 1.35875e-02, Loss_f: 6.12941e-03\n","Iter 551, Loss: 1.96618e-02, Loss_u: 1.35817e-02, Loss_f: 6.08003e-03\n","Iter 552, Loss: 1.96210e-02, Loss_u: 1.35406e-02, Loss_f: 6.08040e-03\n","Iter 553, Loss: 1.95584e-02, Loss_u: 1.35176e-02, Loss_f: 6.04074e-03\n","Iter 554, Loss: 1.95192e-02, Loss_u: 1.35050e-02, Loss_f: 6.01413e-03\n","Iter 555, Loss: 1.94795e-02, Loss_u: 1.36042e-02, Loss_f: 5.87527e-03\n","Iter 556, Loss: 1.93973e-02, Loss_u: 1.35807e-02, Loss_f: 5.81659e-03\n","Iter 557, Loss: 1.92862e-02, Loss_u: 1.33220e-02, Loss_f: 5.96420e-03\n","Iter 558, Loss: 1.92616e-02, Loss_u: 1.35216e-02, Loss_f: 5.73997e-03\n","Iter 559, Loss: 1.92237e-02, Loss_u: 1.34121e-02, Loss_f: 5.81154e-03\n","Iter 560, Loss: 1.91698e-02, Loss_u: 1.34550e-02, Loss_f: 5.71478e-03\n","Iter 561, Loss: 1.91244e-02, Loss_u: 1.33269e-02, Loss_f: 5.79752e-03\n","Iter 562, Loss: 1.91083e-02, Loss_u: 1.34313e-02, Loss_f: 5.67705e-03\n","Iter 563, Loss: 1.90880e-02, Loss_u: 1.29374e-02, Loss_f: 6.15056e-03\n","Iter 564, Loss: 1.89514e-02, Loss_u: 1.30772e-02, Loss_f: 5.87418e-03\n","Iter 565, Loss: 1.89514e-02, Loss_u: 1.30772e-02, Loss_f: 5.87418e-03\n","Iter 566, Loss: 1.89243e-02, Loss_u: 1.34098e-02, Loss_f: 5.51448e-03\n","Iter 567, Loss: 1.88738e-02, Loss_u: 1.32598e-02, Loss_f: 5.61397e-03\n","Iter 568, Loss: 1.88451e-02, Loss_u: 1.32171e-02, Loss_f: 5.62801e-03\n","Iter 569, Loss: 1.88065e-02, Loss_u: 1.31661e-02, Loss_f: 5.64045e-03\n","Iter 570, Loss: 1.87655e-02, Loss_u: 1.31635e-02, Loss_f: 5.60205e-03\n","Iter 571, Loss: 1.87420e-02, Loss_u: 1.31480e-02, Loss_f: 5.59396e-03\n","Iter 572, Loss: 1.87343e-02, Loss_u: 1.29738e-02, Loss_f: 5.76052e-03\n","Iter 573, Loss: 1.86968e-02, Loss_u: 1.31010e-02, Loss_f: 5.59585e-03\n","Iter 574, Loss: 1.86735e-02, Loss_u: 1.31673e-02, Loss_f: 5.50615e-03\n","Iter 575, Loss: 1.86276e-02, Loss_u: 1.32327e-02, Loss_f: 5.39489e-03\n","Iter 576, Loss: 1.86191e-02, Loss_u: 1.34229e-02, Loss_f: 5.19622e-03\n","Iter 577, Loss: 1.85729e-02, Loss_u: 1.33925e-02, Loss_f: 5.18038e-03\n","Iter 578, Loss: 1.85480e-02, Loss_u: 1.32981e-02, Loss_f: 5.24988e-03\n","Iter 579, Loss: 1.84902e-02, Loss_u: 1.32958e-02, Loss_f: 5.19439e-03\n","Iter 580, Loss: 1.84158e-02, Loss_u: 1.31728e-02, Loss_f: 5.24303e-03\n","Iter 581, Loss: 1.83612e-02, Loss_u: 1.33354e-02, Loss_f: 5.02577e-03\n","Iter 582, Loss: 1.82789e-02, Loss_u: 1.30698e-02, Loss_f: 5.20916e-03\n","Iter 583, Loss: 1.82044e-02, Loss_u: 1.29732e-02, Loss_f: 5.23121e-03\n","Iter 584, Loss: 1.81306e-02, Loss_u: 1.28278e-02, Loss_f: 5.30285e-03\n","Iter 585, Loss: 1.81306e-02, Loss_u: 1.28278e-02, Loss_f: 5.30285e-03\n","Iter 586, Loss: 1.80662e-02, Loss_u: 1.27941e-02, Loss_f: 5.27214e-03\n","Iter 587, Loss: 1.79990e-02, Loss_u: 1.26471e-02, Loss_f: 5.35192e-03\n","Iter 588, Loss: 1.79104e-02, Loss_u: 1.25643e-02, Loss_f: 5.34610e-03\n","Iter 589, Loss: 1.78519e-02, Loss_u: 1.24924e-02, Loss_f: 5.35960e-03\n","Iter 590, Loss: 1.78043e-02, Loss_u: 1.25219e-02, Loss_f: 5.28239e-03\n","Iter 591, Loss: 1.77380e-02, Loss_u: 1.25972e-02, Loss_f: 5.14075e-03\n","Iter 592, Loss: 1.76736e-02, Loss_u: 1.26361e-02, Loss_f: 5.03751e-03\n","Iter 593, Loss: 1.76010e-02, Loss_u: 1.26332e-02, Loss_f: 4.96776e-03\n","Iter 594, Loss: 1.75463e-02, Loss_u: 1.25812e-02, Loss_f: 4.96502e-03\n","Iter 595, Loss: 1.74548e-02, Loss_u: 1.25091e-02, Loss_f: 4.94570e-03\n","Iter 596, Loss: 1.74062e-02, Loss_u: 1.22016e-02, Loss_f: 5.20457e-03\n","Iter 597, Loss: 1.78639e-02, Loss_u: 1.19335e-02, Loss_f: 5.93042e-03\n","Iter 598, Loss: 1.73750e-02, Loss_u: 1.21414e-02, Loss_f: 5.23360e-03\n","Iter 599, Loss: 1.73573e-02, Loss_u: 1.22990e-02, Loss_f: 5.05823e-03\n","Iter 600, Loss: 1.73149e-02, Loss_u: 1.22910e-02, Loss_f: 5.02395e-03\n","Iter 601, Loss: 1.72964e-02, Loss_u: 1.22497e-02, Loss_f: 5.04667e-03\n","Iter 602, Loss: 1.72662e-02, Loss_u: 1.22360e-02, Loss_f: 5.03020e-03\n","Iter 603, Loss: 1.71912e-02, Loss_u: 1.22382e-02, Loss_f: 4.95306e-03\n","Iter 604, Loss: 1.73083e-02, Loss_u: 1.20921e-02, Loss_f: 5.21617e-03\n","Iter 605, Loss: 1.71482e-02, Loss_u: 1.21879e-02, Loss_f: 4.96029e-03\n","Iter 606, Loss: 7.77777e-01, Loss_u: 1.21879e-02, Loss_f: 7.65589e-01\n","Iter 607, Loss: 1.07877e+00, Loss_u: 1.37878e-02, Loss_f: 1.06498e+00\n","Iter 608, Loss: 7.77596e-01, Loss_u: 1.21867e-02, Loss_f: 7.65409e-01\n","Iter 609, Loss: 9.07541e+00, Loss_u: 1.85955e-01, Loss_f: 8.88946e+00\n","Iter 610, Loss: 6.81919e-01, Loss_u: 1.29250e-02, Loss_f: 6.68994e-01\n","Iter 611, Loss: 5.17842e-01, Loss_u: 1.52863e-02, Loss_f: 5.02556e-01\n","Iter 612, Loss: 4.53288e-01, Loss_u: 1.76170e-02, Loss_f: 4.35671e-01\n","Iter 613, Loss: 4.53501e-01, Loss_u: 2.19490e-02, Loss_f: 4.31552e-01\n","Iter 614, Loss: 4.39409e-01, Loss_u: 1.96216e-02, Loss_f: 4.19788e-01\n","Iter 615, Loss: 7.11814e+07, Loss_u: 2.98742e+00, Loss_f: 7.11814e+07\n","Iter 616, Loss: 3.28783e+01, Loss_u: 6.84821e+00, Loss_f: 2.60301e+01\n","Iter 617, Loss: 4.38116e-01, Loss_u: 2.00203e-02, Loss_f: 4.18095e-01\n","Iter 618, Loss: 1.28009e+00, Loss_u: 9.42456e-02, Loss_f: 1.18584e+00\n","Iter 619, Loss: 3.97351e-01, Loss_u: 2.27179e-02, Loss_f: 3.74633e-01\n","Iter 620, Loss: 7.90457e-01, Loss_u: 7.28677e-01, Loss_f: 6.17801e-02\n","Iter 621, Loss: 7.30185e-01, Loss_u: 9.67071e-02, Loss_f: 6.33478e-01\n","Iter 622, Loss: 3.75383e-01, Loss_u: 2.93225e-02, Loss_f: 3.46060e-01\n","Iter 623, Loss: 1.83003e+00, Loss_u: 1.85303e-01, Loss_f: 1.64473e+00\n","Iter 624, Loss: 2.74931e-01, Loss_u: 2.86728e-02, Loss_f: 2.46258e-01\n","Iter 625, Loss: 1.28904e+01, Loss_u: 1.52929e+00, Loss_f: 1.13611e+01\n","Iter 626, Loss: 2.36963e-01, Loss_u: 3.07007e-02, Loss_f: 2.06262e-01\n","Iter 627, Loss: 2.36963e-01, Loss_u: 3.07007e-02, Loss_f: 2.06262e-01\n","Iter 628, Loss: 1.02795e+00, Loss_u: 1.21467e-01, Loss_f: 9.06478e-01\n","Iter 629, Loss: 1.79309e-01, Loss_u: 3.25558e-02, Loss_f: 1.46753e-01\n","Iter 630, Loss: 2.23944e+01, Loss_u: 5.83106e-01, Loss_f: 2.18113e+01\n","Iter 631, Loss: 1.55232e-01, Loss_u: 3.10477e-02, Loss_f: 1.24184e-01\n","Iter 632, Loss: 2.75892e-01, Loss_u: 3.01247e-02, Loss_f: 2.45767e-01\n","Iter 633, Loss: 1.30890e-01, Loss_u: 2.96254e-02, Loss_f: 1.01264e-01\n","Iter 634, Loss: 8.46224e-02, Loss_u: 3.52766e-02, Loss_f: 4.93458e-02\n","Iter 635, Loss: 8.63585e-02, Loss_u: 3.98405e-02, Loss_f: 4.65180e-02\n","Iter 636, Loss: 7.67366e-02, Loss_u: 3.60202e-02, Loss_f: 4.07163e-02\n","Iter 637, Loss: 2.20603e-01, Loss_u: 5.96551e-02, Loss_f: 1.60947e-01\n","Iter 638, Loss: 7.22047e-02, Loss_u: 3.85398e-02, Loss_f: 3.36649e-02\n","Iter 639, Loss: 6.72103e-02, Loss_u: 3.63576e-02, Loss_f: 3.08528e-02\n","Iter 640, Loss: 6.87621e-02, Loss_u: 3.46296e-02, Loss_f: 3.41325e-02\n","Iter 641, Loss: 6.37151e-02, Loss_u: 3.51906e-02, Loss_f: 2.85245e-02\n","Iter 642, Loss: 5.97187e-02, Loss_u: 3.29377e-02, Loss_f: 2.67809e-02\n","Iter 643, Loss: 6.42904e-02, Loss_u: 2.44034e-02, Loss_f: 3.98870e-02\n","Iter 644, Loss: 5.53281e-02, Loss_u: 2.81654e-02, Loss_f: 2.71627e-02\n","Iter 645, Loss: 5.03829e-02, Loss_u: 2.70071e-02, Loss_f: 2.33757e-02\n","Iter 646, Loss: 5.12543e-02, Loss_u: 2.83505e-02, Loss_f: 2.29038e-02\n","Iter 647, Loss: 4.85822e-02, Loss_u: 2.72786e-02, Loss_f: 2.13036e-02\n","Iter 648, Loss: 4.85822e-02, Loss_u: 2.72786e-02, Loss_f: 2.13036e-02\n","Iter 649, Loss: 4.45725e-02, Loss_u: 2.77322e-02, Loss_f: 1.68403e-02\n","Iter 650, Loss: 4.81623e-02, Loss_u: 2.98023e-02, Loss_f: 1.83600e-02\n","Iter 651, Loss: 4.37836e-02, Loss_u: 2.84247e-02, Loss_f: 1.53589e-02\n","Iter 652, Loss: 4.25370e-02, Loss_u: 2.66274e-02, Loss_f: 1.59096e-02\n","Iter 653, Loss: 4.05786e-02, Loss_u: 2.45762e-02, Loss_f: 1.60023e-02\n","Iter 654, Loss: 3.85880e-02, Loss_u: 2.12144e-02, Loss_f: 1.73736e-02\n","Iter 655, Loss: 3.70090e-02, Loss_u: 1.92277e-02, Loss_f: 1.77813e-02\n","Iter 656, Loss: 3.47909e-02, Loss_u: 1.76069e-02, Loss_f: 1.71840e-02\n","Iter 657, Loss: 3.36067e-02, Loss_u: 1.58941e-02, Loss_f: 1.77126e-02\n","Iter 658, Loss: 3.29268e-02, Loss_u: 1.67632e-02, Loss_f: 1.61636e-02\n","Iter 659, Loss: 3.27442e-02, Loss_u: 1.75323e-02, Loss_f: 1.52118e-02\n","Iter 660, Loss: 3.23172e-02, Loss_u: 1.70150e-02, Loss_f: 1.53022e-02\n","Iter 661, Loss: 3.14917e-02, Loss_u: 1.64490e-02, Loss_f: 1.50427e-02\n","Iter 662, Loss: 3.08712e-02, Loss_u: 1.64545e-02, Loss_f: 1.44168e-02\n","Iter 663, Loss: 3.05014e-02, Loss_u: 1.77272e-02, Loss_f: 1.27742e-02\n","Iter 664, Loss: 3.01865e-02, Loss_u: 1.74372e-02, Loss_f: 1.27493e-02\n","Iter 665, Loss: 3.00539e-02, Loss_u: 1.72482e-02, Loss_f: 1.28058e-02\n","Iter 666, Loss: 2.97696e-02, Loss_u: 1.72007e-02, Loss_f: 1.25689e-02\n","Iter 667, Loss: 2.94280e-02, Loss_u: 1.73272e-02, Loss_f: 1.21008e-02\n","Iter 668, Loss: 2.94280e-02, Loss_u: 1.73272e-02, Loss_f: 1.21008e-02\n","Iter 669, Loss: 2.90014e-02, Loss_u: 1.75204e-02, Loss_f: 1.14810e-02\n","Iter 670, Loss: 2.85657e-02, Loss_u: 1.76900e-02, Loss_f: 1.08757e-02\n","Iter 671, Loss: 2.82356e-02, Loss_u: 1.79001e-02, Loss_f: 1.03355e-02\n","Iter 672, Loss: 2.79709e-02, Loss_u: 1.78993e-02, Loss_f: 1.00716e-02\n","Iter 673, Loss: 2.76831e-02, Loss_u: 1.79546e-02, Loss_f: 9.72849e-03\n","Iter 674, Loss: 2.73331e-02, Loss_u: 1.81186e-02, Loss_f: 9.21449e-03\n","Iter 675, Loss: 2.70630e-02, Loss_u: 1.85285e-02, Loss_f: 8.53446e-03\n","Iter 676, Loss: 2.69131e-02, Loss_u: 1.86273e-02, Loss_f: 8.28585e-03\n","Iter 677, Loss: 2.68147e-02, Loss_u: 1.85217e-02, Loss_f: 8.29303e-03\n","Iter 678, Loss: 2.66567e-02, Loss_u: 1.81069e-02, Loss_f: 8.54975e-03\n","Iter 679, Loss: 2.64767e-02, Loss_u: 1.78082e-02, Loss_f: 8.66850e-03\n","Iter 680, Loss: 2.63427e-02, Loss_u: 1.72497e-02, Loss_f: 9.09300e-03\n","Iter 681, Loss: 2.62352e-02, Loss_u: 1.74378e-02, Loss_f: 8.79737e-03\n","Iter 682, Loss: 2.63033e-02, Loss_u: 1.72400e-02, Loss_f: 9.06334e-03\n","Iter 683, Loss: 2.62026e-02, Loss_u: 1.73575e-02, Loss_f: 8.84515e-03\n","Iter 684, Loss: 2.67146e-02, Loss_u: 1.57473e-02, Loss_f: 1.09673e-02\n","Iter 685, Loss: 2.60833e-02, Loss_u: 1.67410e-02, Loss_f: 9.34233e-03\n","Iter 686, Loss: 2.62099e-02, Loss_u: 1.69687e-02, Loss_f: 9.24117e-03\n","Iter 687, Loss: 2.60447e-02, Loss_u: 1.67937e-02, Loss_f: 9.25098e-03\n","Iter 688, Loss: 2.60447e-02, Loss_u: 1.67937e-02, Loss_f: 9.25098e-03\n","Iter 689, Loss: 2.62907e-02, Loss_u: 1.66918e-02, Loss_f: 9.59897e-03\n","Iter 690, Loss: 2.59741e-02, Loss_u: 1.67393e-02, Loss_f: 9.23481e-03\n","Iter 691, Loss: 2.59453e-02, Loss_u: 1.69415e-02, Loss_f: 9.00382e-03\n","Iter 692, Loss: 2.58108e-02, Loss_u: 1.68620e-02, Loss_f: 8.94881e-03\n","Iter 693, Loss: 2.57736e-02, Loss_u: 1.66877e-02, Loss_f: 9.08587e-03\n","Iter 694, Loss: 2.57007e-02, Loss_u: 1.66784e-02, Loss_f: 9.02230e-03\n","Iter 695, Loss: 2.59002e-02, Loss_u: 1.54122e-02, Loss_f: 1.04879e-02\n","Iter 696, Loss: 2.55843e-02, Loss_u: 1.61210e-02, Loss_f: 9.46334e-03\n","Iter 697, Loss: 2.54511e-02, Loss_u: 1.62850e-02, Loss_f: 9.16610e-03\n","Iter 698, Loss: 2.53302e-02, Loss_u: 1.64471e-02, Loss_f: 8.88306e-03\n","Iter 699, Loss: 2.52339e-02, Loss_u: 1.64636e-02, Loss_f: 8.77031e-03\n","Iter 700, Loss: 2.50939e-02, Loss_u: 1.62224e-02, Loss_f: 8.87151e-03\n","Iter 701, Loss: 2.53860e-02, Loss_u: 1.55374e-02, Loss_f: 9.84859e-03\n","Iter 702, Loss: 2.50130e-02, Loss_u: 1.59563e-02, Loss_f: 9.05676e-03\n","Iter 703, Loss: 2.49448e-02, Loss_u: 1.55357e-02, Loss_f: 9.40912e-03\n","Iter 704, Loss: 2.48872e-02, Loss_u: 1.57721e-02, Loss_f: 9.11509e-03\n","Iter 705, Loss: 2.48416e-02, Loss_u: 1.58639e-02, Loss_f: 8.97768e-03\n","Iter 706, Loss: 2.47630e-02, Loss_u: 1.59845e-02, Loss_f: 8.77856e-03\n","Iter 707, Loss: 2.45740e-02, Loss_u: 1.63206e-02, Loss_f: 8.25346e-03\n","Iter 708, Loss: 2.45740e-02, Loss_u: 1.63206e-02, Loss_f: 8.25346e-03\n","Iter 709, Loss: 2.44447e-02, Loss_u: 1.63097e-02, Loss_f: 8.13501e-03\n","Iter 710, Loss: 2.43562e-02, Loss_u: 1.65591e-02, Loss_f: 7.79711e-03\n","Iter 711, Loss: 2.43039e-02, Loss_u: 1.63580e-02, Loss_f: 7.94588e-03\n","Iter 712, Loss: 2.42428e-02, Loss_u: 1.60082e-02, Loss_f: 8.23456e-03\n","Iter 713, Loss: 2.41901e-02, Loss_u: 1.58234e-02, Loss_f: 8.36674e-03\n","Iter 714, Loss: 2.41341e-02, Loss_u: 1.58129e-02, Loss_f: 8.32117e-03\n","Iter 715, Loss: 2.41084e-02, Loss_u: 1.58250e-02, Loss_f: 8.28339e-03\n","Iter 716, Loss: 2.40864e-02, Loss_u: 1.58708e-02, Loss_f: 8.21567e-03\n","Iter 717, Loss: 2.40515e-02, Loss_u: 1.58426e-02, Loss_f: 8.20891e-03\n","Iter 718, Loss: 2.39618e-02, Loss_u: 1.56554e-02, Loss_f: 8.30639e-03\n","Iter 719, Loss: 2.39018e-02, Loss_u: 1.56313e-02, Loss_f: 8.27045e-03\n","Iter 720, Loss: 2.38544e-02, Loss_u: 1.56694e-02, Loss_f: 8.18497e-03\n","Iter 721, Loss: 2.38188e-02, Loss_u: 1.58184e-02, Loss_f: 8.00037e-03\n","Iter 722, Loss: 2.37897e-02, Loss_u: 1.57725e-02, Loss_f: 8.01720e-03\n","Iter 723, Loss: 2.37464e-02, Loss_u: 1.58649e-02, Loss_f: 7.88153e-03\n","Iter 724, Loss: 2.37023e-02, Loss_u: 1.58531e-02, Loss_f: 7.84917e-03\n","Iter 725, Loss: 2.36488e-02, Loss_u: 1.56813e-02, Loss_f: 7.96754e-03\n","Iter 726, Loss: 2.35968e-02, Loss_u: 1.55944e-02, Loss_f: 8.00234e-03\n","Iter 727, Loss: 2.35648e-02, Loss_u: 1.55030e-02, Loss_f: 8.06183e-03\n","Iter 728, Loss: 2.35648e-02, Loss_u: 1.55030e-02, Loss_f: 8.06183e-03\n","Iter 729, Loss: 2.35399e-02, Loss_u: 1.54450e-02, Loss_f: 8.09493e-03\n","Iter 730, Loss: 2.35260e-02, Loss_u: 1.54588e-02, Loss_f: 8.06720e-03\n","Iter 731, Loss: 2.34953e-02, Loss_u: 1.54804e-02, Loss_f: 8.01485e-03\n","Iter 732, Loss: 2.34362e-02, Loss_u: 1.54750e-02, Loss_f: 7.96122e-03\n","Iter 733, Loss: 2.33567e-02, Loss_u: 1.53850e-02, Loss_f: 7.97169e-03\n","Iter 734, Loss: 2.32497e-02, Loss_u: 1.53016e-02, Loss_f: 7.94811e-03\n","Iter 735, Loss: 2.31856e-02, Loss_u: 1.51935e-02, Loss_f: 7.99206e-03\n","Iter 736, Loss: 2.32955e-02, Loss_u: 1.47481e-02, Loss_f: 8.54742e-03\n","Iter 737, Loss: 2.30775e-02, Loss_u: 1.49680e-02, Loss_f: 8.10952e-03\n","Iter 738, Loss: 2.29274e-02, Loss_u: 1.48039e-02, Loss_f: 8.12357e-03\n","Iter 739, Loss: 2.28171e-02, Loss_u: 1.47554e-02, Loss_f: 8.06172e-03\n","Iter 740, Loss: 2.26796e-02, Loss_u: 1.43281e-02, Loss_f: 8.35148e-03\n","Iter 741, Loss: 2.25957e-02, Loss_u: 1.41849e-02, Loss_f: 8.41076e-03\n","Iter 742, Loss: 2.25028e-02, Loss_u: 1.40031e-02, Loss_f: 8.49966e-03\n","Iter 743, Loss: 2.24187e-02, Loss_u: 1.40593e-02, Loss_f: 8.35949e-03\n","Iter 744, Loss: 2.23518e-02, Loss_u: 1.40963e-02, Loss_f: 8.25550e-03\n","Iter 745, Loss: 2.22865e-02, Loss_u: 1.41678e-02, Loss_f: 8.11870e-03\n","Iter 746, Loss: 2.21792e-02, Loss_u: 1.43868e-02, Loss_f: 7.79237e-03\n","Iter 747, Loss: 2.20861e-02, Loss_u: 1.43434e-02, Loss_f: 7.74264e-03\n","Iter 748, Loss: 2.20861e-02, Loss_u: 1.43434e-02, Loss_f: 7.74264e-03\n","Iter 749, Loss: 2.19722e-02, Loss_u: 1.48196e-02, Loss_f: 7.15257e-03\n","Iter 750, Loss: 2.18851e-02, Loss_u: 1.44009e-02, Loss_f: 7.48417e-03\n","Iter 751, Loss: 2.18248e-02, Loss_u: 1.42463e-02, Loss_f: 7.57848e-03\n","Iter 752, Loss: 2.17636e-02, Loss_u: 1.40167e-02, Loss_f: 7.74693e-03\n","Iter 753, Loss: 2.17023e-02, Loss_u: 1.39135e-02, Loss_f: 7.78884e-03\n","Iter 754, Loss: 2.16164e-02, Loss_u: 1.38667e-02, Loss_f: 7.74967e-03\n","Iter 755, Loss: 2.15365e-02, Loss_u: 1.36801e-02, Loss_f: 7.85641e-03\n","Iter 756, Loss: 2.14996e-02, Loss_u: 1.36149e-02, Loss_f: 7.88474e-03\n","Iter 757, Loss: 2.14757e-02, Loss_u: 1.33447e-02, Loss_f: 8.13102e-03\n","Iter 758, Loss: 2.14352e-02, Loss_u: 1.33502e-02, Loss_f: 8.08507e-03\n","Iter 759, Loss: 2.13965e-02, Loss_u: 1.32663e-02, Loss_f: 8.13024e-03\n","Iter 760, Loss: 2.13497e-02, Loss_u: 1.33175e-02, Loss_f: 8.03224e-03\n","Iter 761, Loss: 2.13016e-02, Loss_u: 1.34086e-02, Loss_f: 7.89304e-03\n","Iter 762, Loss: 2.12263e-02, Loss_u: 1.33806e-02, Loss_f: 7.84572e-03\n","Iter 763, Loss: 2.11567e-02, Loss_u: 1.35311e-02, Loss_f: 7.62557e-03\n","Iter 764, Loss: 2.10937e-02, Loss_u: 1.36461e-02, Loss_f: 7.44760e-03\n","Iter 765, Loss: 2.10571e-02, Loss_u: 1.35734e-02, Loss_f: 7.48365e-03\n","Iter 766, Loss: 2.10257e-02, Loss_u: 1.36013e-02, Loss_f: 7.42437e-03\n","Iter 767, Loss: 2.09880e-02, Loss_u: 1.34900e-02, Loss_f: 7.49801e-03\n","Iter 768, Loss: 2.09880e-02, Loss_u: 1.34900e-02, Loss_f: 7.49801e-03\n","Iter 769, Loss: 2.09577e-02, Loss_u: 1.35518e-02, Loss_f: 7.40598e-03\n","Iter 770, Loss: 2.09243e-02, Loss_u: 1.36524e-02, Loss_f: 7.27191e-03\n","Iter 771, Loss: 2.08867e-02, Loss_u: 1.37093e-02, Loss_f: 7.17736e-03\n","Iter 772, Loss: 2.08444e-02, Loss_u: 1.37501e-02, Loss_f: 7.09432e-03\n","Iter 773, Loss: 2.07979e-02, Loss_u: 1.37290e-02, Loss_f: 7.06886e-03\n","Iter 774, Loss: 2.07576e-02, Loss_u: 1.36081e-02, Loss_f: 7.14947e-03\n","Iter 775, Loss: 2.06996e-02, Loss_u: 1.34922e-02, Loss_f: 7.20743e-03\n","Iter 776, Loss: 2.06428e-02, Loss_u: 1.33378e-02, Loss_f: 7.30502e-03\n","Iter 777, Loss: 2.05703e-02, Loss_u: 1.32025e-02, Loss_f: 7.36788e-03\n","Iter 778, Loss: 2.04963e-02, Loss_u: 1.31645e-02, Loss_f: 7.33185e-03\n","Iter 779, Loss: 2.05682e-02, Loss_u: 1.29302e-02, Loss_f: 7.63801e-03\n","Iter 780, Loss: 2.04526e-02, Loss_u: 1.30668e-02, Loss_f: 7.38576e-03\n","Iter 781, Loss: 2.03800e-02, Loss_u: 1.30632e-02, Loss_f: 7.31678e-03\n","Iter 782, Loss: 2.03051e-02, Loss_u: 1.29115e-02, Loss_f: 7.39361e-03\n","Iter 783, Loss: 2.03254e-02, Loss_u: 1.29883e-02, Loss_f: 7.33708e-03\n","Iter 784, Loss: 2.02484e-02, Loss_u: 1.29320e-02, Loss_f: 7.31644e-03\n","Iter 785, Loss: 2.01520e-02, Loss_u: 1.30664e-02, Loss_f: 7.08554e-03\n","Iter 786, Loss: 2.00552e-02, Loss_u: 1.32650e-02, Loss_f: 6.79019e-03\n","Iter 787, Loss: 1.99998e-02, Loss_u: 1.34074e-02, Loss_f: 6.59232e-03\n","Iter 788, Loss: 1.99998e-02, Loss_u: 1.34074e-02, Loss_f: 6.59232e-03\n","Iter 789, Loss: 1.99416e-02, Loss_u: 1.34550e-02, Loss_f: 6.48661e-03\n","Iter 790, Loss: 1.98791e-02, Loss_u: 1.33882e-02, Loss_f: 6.49098e-03\n","Iter 791, Loss: 1.98010e-02, Loss_u: 1.32971e-02, Loss_f: 6.50397e-03\n","Iter 792, Loss: 1.96790e-02, Loss_u: 1.31598e-02, Loss_f: 6.51922e-03\n","Iter 793, Loss: 1.95812e-02, Loss_u: 1.30198e-02, Loss_f: 6.56143e-03\n","Iter 794, Loss: 1.95664e-02, Loss_u: 1.30222e-02, Loss_f: 6.54427e-03\n","Iter 795, Loss: 1.95109e-02, Loss_u: 1.30154e-02, Loss_f: 6.49556e-03\n","Iter 796, Loss: 1.94552e-02, Loss_u: 1.31064e-02, Loss_f: 6.34884e-03\n","Iter 797, Loss: 1.94182e-02, Loss_u: 1.31279e-02, Loss_f: 6.29036e-03\n","Iter 798, Loss: 1.93848e-02, Loss_u: 1.31619e-02, Loss_f: 6.22294e-03\n","Iter 799, Loss: 1.93575e-02, Loss_u: 1.31788e-02, Loss_f: 6.17873e-03\n","Iter 800, Loss: 1.93397e-02, Loss_u: 1.31963e-02, Loss_f: 6.14333e-03\n","Iter 801, Loss: 1.93020e-02, Loss_u: 1.31944e-02, Loss_f: 6.10756e-03\n","Iter 802, Loss: 1.92660e-02, Loss_u: 1.31554e-02, Loss_f: 6.11061e-03\n","Iter 803, Loss: 1.92339e-02, Loss_u: 1.30749e-02, Loss_f: 6.15900e-03\n","Iter 804, Loss: 1.91981e-02, Loss_u: 1.30123e-02, Loss_f: 6.18580e-03\n","Iter 805, Loss: 1.91604e-02, Loss_u: 1.28792e-02, Loss_f: 6.28125e-03\n","Iter 806, Loss: 1.91196e-02, Loss_u: 1.27939e-02, Loss_f: 6.32567e-03\n","Iter 807, Loss: 1.90636e-02, Loss_u: 1.26412e-02, Loss_f: 6.42248e-03\n","Iter 808, Loss: 1.51832e+01, Loss_u: 1.26412e-02, Loss_f: 1.51706e+01\n","Iter 809, Loss: 6.53100e+02, Loss_u: 4.02856e-02, Loss_f: 6.53060e+02\n","Iter 810, Loss: 3.66184e+01, Loss_u: 1.23094e-02, Loss_f: 3.66061e+01\n","Iter 811, Loss: 1.52058e+01, Loss_u: 1.22977e-02, Loss_f: 1.51935e+01\n","Iter 812, Loss: 1.51778e+01, Loss_u: 1.25200e-02, Loss_f: 1.51653e+01\n","Iter 813, Loss: 8.19186e+07, Loss_u: 2.22414e+04, Loss_f: 8.18964e+07\n","Iter 814, Loss: 7.59563e+18, Loss_u: 9.00878e+03, Loss_f: 7.59563e+18\n","Iter 815, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 816, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 817, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 818, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 819, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 820, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 821, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 822, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 823, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 824, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 825, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 826, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 827, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 828, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 829, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 830, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 831, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 832, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 833, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 834, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 835, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 836, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 837, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 838, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 839, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 840, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 841, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 842, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 843, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 844, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 845, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 846, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 847, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 848, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 849, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 850, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 851, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 852, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 853, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 854, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 855, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 856, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 857, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 858, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 859, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 860, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 861, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 862, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 863, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 864, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 865, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 866, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 867, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 868, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 869, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 870, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 871, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 872, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 873, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 874, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 875, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 876, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 877, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 878, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 879, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 880, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 881, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 882, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 883, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 884, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 885, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 886, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 887, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 888, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 889, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 890, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 891, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 892, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 893, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 894, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 895, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 896, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 897, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 898, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 899, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 900, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 901, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 902, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 903, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 904, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 905, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 906, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 907, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 908, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 909, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 910, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 911, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 912, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 913, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 914, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 915, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 916, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 917, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 918, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 919, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 920, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 921, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 922, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 923, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 924, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 925, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 926, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 927, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 928, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 929, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 930, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 931, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 932, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 933, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 934, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 935, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 936, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 937, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 938, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 939, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 940, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 941, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 942, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 943, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 944, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 945, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 946, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 947, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 948, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 949, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 950, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 951, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 952, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 953, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 954, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 955, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 956, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 957, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 958, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 959, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 960, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 961, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 962, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 963, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 964, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 965, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 966, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 967, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 968, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 969, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 970, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 971, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 972, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 973, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 974, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 975, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 976, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 977, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 978, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 979, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 980, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 981, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 982, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 983, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 984, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 985, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 986, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 987, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 988, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 989, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 990, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 991, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 992, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 993, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 994, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 995, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 996, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 997, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 998, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 999, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1000, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1001, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1002, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1003, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1004, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1005, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1006, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1007, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1008, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1009, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1010, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1011, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1012, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1013, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1014, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1015, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1016, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1017, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1018, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1019, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1020, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1021, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1022, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1023, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1024, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1025, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1026, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1027, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1028, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1029, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1030, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1031, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1032, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1033, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1034, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1035, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1036, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1037, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1038, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1039, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1040, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1041, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1042, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1043, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1044, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1045, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1046, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1047, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1048, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1049, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1050, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1051, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1052, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1053, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1054, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1055, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1056, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1057, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1058, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1059, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1060, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1061, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1062, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1063, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1064, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1065, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1066, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1067, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1068, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1069, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1070, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1071, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1072, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1073, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1074, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1075, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1076, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1077, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1078, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1079, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1080, Loss: nan, Loss_u: nan, Loss_f: nan\n","Iter 1081, Loss: nan, Loss_u: nan, Loss_f: nan\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;32m<ipython-input-18-b984681c28a2>\u001b[0m in \u001b[0;36madaptive_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;31m# sample (1-self.eta)*Nf points of xf using proxy function Pi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0mx_f2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_f2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_f2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-b984681c28a2>\u001b[0m in \u001b[0;36msample_proxy\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0msample_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: probabilities contain NaN"]}],"source":["%%time\n","\n","model.adaptive_train()"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":461},"executionInfo":{"elapsed":16,"status":"error","timestamp":1666152722112,"user":{"displayName":"Nageswar Venkata Sai Gangadhar","userId":"02855835275532238793"},"user_tz":-330},"id":"FMZoVK7SiRmZ","outputId":"02ccc1c7-f704-41ec-fb57-a76dbc9f2f8a"},"outputs":[{"output_type":"error","ename":"LinAlgError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-e0abcf71ed69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0merror_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_star\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_star\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error u: %e'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merror_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2577\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Duplicate axes given.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mord\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2579\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0m_multi_svd_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2580\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mord\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2581\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_multi_svd_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_multi_svd_norm\u001b[0;34m(x, row_axis, col_axis, op)\u001b[0m\n\u001b[1;32m   2353\u001b[0m     \"\"\"\n\u001b[1;32m   2354\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmoveaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_uv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->d'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1673\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_svd_nonconvergence\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_svd_nonconvergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVD did not converge\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_lstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLinAlgError\u001b[0m: SVD did not converge"]}],"source":["u_pred, f_pred = model.predict(X_star)\n","\n","error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n","print('Error u: %e' % (error_u))                     \n","\n","U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n","Error = np.abs(Exact - U_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"aborted","timestamp":1666152722112,"user":{"displayName":"Nageswar Venkata Sai Gangadhar","userId":"02855835275532238793"},"user_tz":-330},"id":"dVUSWFvMiRqN"},"outputs":[],"source":["\"\"\" The aesthetic setting has changed. \"\"\"\n","\n","####### Row 0: u(t,x) ##################    \n","\n","fig = plt.figure(figsize=(9, 5))\n","ax = fig.add_subplot(111)\n","\n","h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n","              extent=[t.min(), t.max(), x.min(), x.max()], \n","              origin='lower', aspect='auto')\n","divider = make_axes_locatable(ax)\n","cax = divider.append_axes(\"right\", size=\"5%\", pad=0.10)\n","cbar = fig.colorbar(h, cax=cax)\n","cbar.ax.tick_params(labelsize=15) \n","\n","ax.plot(\n","    X_u_train[:,1], \n","    X_u_train[:,0], \n","    'kx', label = 'Data (%d points)' % (u_train.shape[0]), \n","    markersize = 4,  # marker size doubled\n","    clip_on = False,\n","    alpha=1.0\n",")\n","\n","line = np.linspace(x.min(), x.max(), 2)[:,None]\n","ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n","ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n","ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n","\n","ax.set_xlabel('$t$', size=20)\n","ax.set_ylabel('$x$', size=20)\n","ax.legend(\n","    loc='upper center', \n","    bbox_to_anchor=(0.9, -0.05), \n","    ncol=5, \n","    frameon=False, \n","    prop={'size': 15}\n",")\n","ax.set_title('$u(t,x)$', fontsize = 20) # font size doubled\n","ax.tick_params(labelsize=15)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"jbhD5EASjH0x"},"source":["# **Visualizations**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1666152722113,"user":{"displayName":"Nageswar Venkata Sai Gangadhar","userId":"02855835275532238793"},"user_tz":-330},"id":"lKWSK9wYiRtl"},"outputs":[],"source":["####### Row 1: u(t,x) slices ################## \n","\n","\"\"\" The aesthetic setting has changed. \"\"\"\n","\n","fig = plt.figure(figsize=(14, 10))\n","ax = fig.add_subplot(111)\n","\n","gs1 = gridspec.GridSpec(1, 3)\n","gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n","\n","ax = plt.subplot(gs1[0, 0])\n","ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \n","ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n","ax.set_xlabel('$x$')\n","ax.set_ylabel('$u(t,x)$')    \n","ax.set_title('$t = 0.25$', fontsize = 15)\n","ax.axis('square')\n","ax.set_xlim([-1.1,1.1])\n","ax.set_ylim([-1.1,1.1])\n","\n","for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n","             ax.get_xticklabels() + ax.get_yticklabels()):\n","    item.set_fontsize(15)\n","\n","ax = plt.subplot(gs1[0, 1])\n","ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \n","ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n","ax.set_xlabel('$x$')\n","ax.set_ylabel('$u(t,x)$')\n","ax.axis('square')\n","ax.set_xlim([-1.1,1.1])\n","ax.set_ylim([-1.1,1.1])\n","ax.set_title('$t = 0.50$', fontsize = 15)\n","ax.legend(\n","    loc='upper center', \n","    bbox_to_anchor=(0.5, -0.15), \n","    ncol=5, \n","    frameon=False, \n","    prop={'size': 15}\n",")\n","\n","for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n","             ax.get_xticklabels() + ax.get_yticklabels()):\n","    item.set_fontsize(15)\n","\n","ax = plt.subplot(gs1[0, 2])\n","ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \n","ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n","ax.set_xlabel('$x$')\n","ax.set_ylabel('$u(t,x)$')\n","ax.axis('square')\n","ax.set_xlim([-1.1,1.1])\n","ax.set_ylim([-1.1,1.1])    \n","ax.set_title('$t = 0.75$', fontsize = 15)\n","\n","for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n","             ax.get_xticklabels() + ax.get_yticklabels()):\n","    item.set_fontsize(15)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1666152722113,"user":{"displayName":"Nageswar Venkata Sai Gangadhar","userId":"02855835275532238793"},"user_tz":-330},"id":"PkRy6dQc-Gyk"},"outputs":[],"source":["import matplotlib\n","\n","x_axis = [i for i in range(1,len(train_error)+1)]\n","\n","new_train_error = np.log(train_error)\n","plt.plot(x_axis, new_train_error)\n","# plt.ylim(1e-4, 1)\n","plt.title(\"train_error error\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1666152722113,"user":{"displayName":"Nageswar Venkata Sai Gangadhar","userId":"02855835275532238793"},"user_tz":-330},"id":"LCzyLm5YO3SB"},"outputs":[],"source":["x_col = model.x_f.detach().cpu().numpy()\n","t_col = model.t_f.detach().cpu().numpy()\n","\n","plt.scatter(x_col, t_col)\n","plt.title(\"collocation points: {}- vanilla\".format(N_f))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}